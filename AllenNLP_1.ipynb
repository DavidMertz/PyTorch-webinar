{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing with AllenNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\">What is AllenNLP?</font>\n",
    "<a href=\"AllenNLP_0.ipynb\"><img src=\"img/open-notebook.png\" align=\"right\"/></a>\n",
    "\n",
    "<font size=\"+1\"><u><b>What is SpaCy?</b></u></font>\n",
    "<a href=\"AllenNLP_1.ipynb\"><img src=\"img/open-notebook.png\" align=\"right\"/></a>\n",
    "\n",
    "<font size=\"+1\">Hight Level Interfaces to NLP using PyTorch</font>\n",
    "<a href=\"AllenNLP_2.ipynb\"><img src=\"img/open-notebook.png\" align=\"right\"/></a>\n",
    "\n",
    "<font size=\"+1\">Sentiment Analysis</font>\n",
    "<a href=\"AllenNLP_3.ipynb\"><img src=\"img/open-notebook.png\" align=\"right\"/></a>\n",
    "\n",
    "<font size=\"+1\">Part-of-Speech Tagging</font> \n",
    "<a href=\"AllenNLP_4.ipynb\"><img src=\"img/open-notebook.png\" align=\"right\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## What is SpaCy?\n",
    "\n",
    "SpaCy starts to get us a bit further afield of this tutorial on PyTorch.  In different ways, PyTorch and SpaCy form the main building blocks for AllenNLP.  Although having different design philosophies, SpaCy occupies a similar space to the libraries [NLTK (Natural Language Toolkit)](https://github.com/nltk/nltk) and [CoreNLP](https://stanfordnlp.github.io/CoreNLP/).\n",
    "\n",
    "In SpaCy, you can build your own linguistic models, but it also comes with built models and high-level tools with features such as:\n",
    "\n",
    "* Non-destructive tokenization\n",
    "* Named entity recognition\n",
    "* Support for 49+ languages\n",
    "* 16 statistical models for 9 languages\n",
    "* Pre-trained word vectors\n",
    "* Easy deep learning integration\n",
    "* Part-of-speech tagging\n",
    "* Labelled dependency parsing\n",
    "* Syntax-driven sentence segmentation\n",
    "* Built in visualizers for syntax and NER\n",
    "* Convenient string-to-hash mapping\n",
    "* Export to numpy data arrays\n",
    "* Efficient binary serialization\n",
    "* Easy model packaging and deployment\n",
    "* Robust, rigorously evaluated accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple examples\n",
    "\n",
    "These are taken from the SpaCy website and documentation with minimal changes, just for illustration.  Built-in tools do many of the things we demonstrate customer neural network models for.  The two sides play well together; the general problems are well solved, but we can build on the basics with PyTorch, SpaCy, and AllenNLP to create more customized models and tools.\n",
    "\n",
    "The `en_core_web_sm` model builds in many useful analyses of English sentences.  Similar models exist for other languages.  SpaCy has an interesting approach of providing one monolithic core model that serves many purposes.  Hence you often start working with the library by creating a general `nlp` object that can analyze text in multiple ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process whole documents\n",
    "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        \"Google in 2007, few people outside of the company took him \"\n",
    "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
    "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
    "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
    "        \"this week.\")\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze syntax\n",
    "print(\"Noun phrases:\\n-\", \"\\n- \".join(chunk.text for chunk in doc.noun_chunks))\n",
    "\n",
    "print(\"\\nVerbs:\\n-\", \"\\n- \".join(token.lemma_ for token in doc if token.pos_ == \"VERB\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text.ljust(18), \"|\", entity.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "Roughly the same thing as *stemming* (but utilizing contextual clues better), lemmatization lets us find the root forms of words. Depending on purposes, it can often be useful to reduce the feature set in a vocabulary or \"bag of words\" in order to simplify modeling.  For example, if you wished to identify conceptual areas addressed in a text, dealing with many declensional forms of words is probably simply overhead and noise.  Models of the sort I mention often address *topicality*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "\n",
    "# Parse the sentence using the loaded 'en' model object `nlp`\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Extract the lemma for each token and join\n",
    "\" \".join([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp('Apple is looking at buying U.K. startup for $1 billion')\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame([(t.text, t.lemma_, t.pos_, t.tag_, t.dep_,\n",
    "               t.shape_, t.is_alpha, t.is_stop) for t in doc],\n",
    "             columns=['Text', 'Lemma', \"Pos\", \"Tag\", \"Dep\", \n",
    "                      \"Shape\", \"is_alpha\", \"is_stop\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
