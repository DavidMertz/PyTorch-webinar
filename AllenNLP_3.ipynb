{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing with AllenNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\">What is AllenNLP?</font>\n",
    "<a href=\"AllenNLP_0.ipynb\"><img src=\"img/open-notebook.png\" align=\"right\"/></a>\n",
    "\n",
    "<font size=\"+1\">What is SpaCy?</font>\n",
    "<a href=\"AllenNLP_1.ipynb\"><img src=\"img/open-notebook.png\" align=\"right\"/></a>\n",
    "\n",
    "<font size=\"+1\">High Level Interfaces to NLP using PyTorch</font>\n",
    "<a href=\"AllenNLP_2.ipynb\"><img src=\"img/open-notebook.png\" align=\"right\"/></a>\n",
    "\n",
    "<font size=\"+1\"><u><b>Sentiment Analysis</b></u></font>\n",
    "<a href=\"AllenNLP_3.ipynb\"><img src=\"img/open-notebook.png\" align=\"right\"/></a>\n",
    "\n",
    "<font size=\"+1\">Part-of-Speech Tagging</font> \n",
    "<a href=\"AllenNLP_4.ipynb\"><img src=\"img/open-notebook.png\" align=\"right\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a minor matter, a number of functions in AllenNLP echo progress messages to STDERR in a way I find distracting for these lessons.  We can stash them in a log file instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import redirect_stderr\n",
    "log = open('stderr.log', 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also check for CUDA, which will make things run much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    device = 0\n",
    "    print(torch.cuda.get_device_name(device))\n",
    "    print(f\"GPU memory used: {torch.cuda.memory_allocated(device):,}\")\n",
    "else:\n",
    "    device = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credit\n",
    "\n",
    "The material in this lesson is borrowed from Masato Hagiwara's [Training a Sentiment Analyzer using AllenNLP (in less than 100 lines of Python code)](http://www.realworldnlpbook.com/blog/training-sentiment-analyzer-using-allennlp.html).  I have made some minor changes to the code and provided my own commentary; I recommend all of his blog posts and other writing highly. I am very much looking forward to the release of his book _Real-World Natural Language Processing_, to be published in 2019 by Manning.\n",
    "\n",
    "The dataset used in this example by Hagiwara, and hence by me, are provided by Stanford University's [Sentiment Analysis](https://nlp.stanford.edu/sentiment/index.html) research page.  This dataset was \n",
    "presented in the paper _Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank_ by Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher Manning, Andrew Ng and Christopher Potts.  The dataset is provided with this repository for convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment tree\n",
    "\n",
    "It is worthwhile to understand what the sentiment tree contains.  If we were only to assign sentiment values to single words, we would often miss the larger structure of overall sentence.  This of the famous saying popularly misattributed to Samual Johnson:\n",
    "\n",
    "> Your manuscript is both good and original, but the part that is good is not original and the part that is original is not good\n",
    "\n",
    "Every individual word in that has a positive or neutral sentiment, but overall it is a scalding criticism.  We can see whether our model identifies this example, but in general we want to look for larger phrases.\n",
    "\n",
    "The Stanford dataset tags arbitrarily long phrases as well as individual words.  It uses 5-levels of sentiment, but the reader could be parameterized to use 3-level or 2-level by simplification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "training = open('data/stanford/train.txt').readlines()\n",
    "print(\"Num samples:\", len(training))\n",
    "samp = training[21].strip()\n",
    "print(\"Example:    \", samp)\n",
    "print(\"Unadorned:  \", \n",
    "      ' '.join(re.sub(r'[012345()]', '', samp).split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to be sure to include the subtrees here.  I.e. we want to utilize the tagged sentiment of phrases, not only of individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# AllenNLP comes with a reader for this format \n",
    "from allennlp.data.dataset_readers import stanford_sentiment_tree_bank\n",
    "# The names for objects are rather long, use an abbrev for single use\n",
    "SSTBDR = stanford_sentiment_tree_bank.StanfordSentimentTreeBankDatasetReader\n",
    "reader = SSTBDR(granularity='5-class', use_subtrees=True)\n",
    "\n",
    "with redirect_stderr(log):\n",
    "    train_dataset = reader.read('data/stanford/train.txt')\n",
    "    dev_dataset = reader.read('data/stanford/dev.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    tokens = train_dataset[i]['tokens']\n",
    "    label = train_dataset[i]['label']\n",
    "    print(tokens) \n",
    "    print(label, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "\n",
    "We also need to encode the vocabulary of the training set as integers.  The `Vocabulary` class provides a numerous features for exactly how this is accomplished.  For example, below we disregard any words that occur fewer than three times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.vocabulary import Vocabulary\n",
    "vocab = Vocabulary.from_instances(train_dataset + dev_dataset,\n",
    "                                  min_count={'tokens': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(15):\n",
    "    print(vocab.get_token_from_index(i), end=' | ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding the vocabulary into tensors\n",
    "\n",
    "We need to represent words in the vocabulary as vectors/tensors into a much less dimensional space than, for example, a one-hot encoding of all the words in the vocabulary.  Each word is mapped to one vector.  Moreover, in this embedding, the transform learns to give words that are used in similar ways comparatively similar vectors, thereby capturing their similarity.\n",
    "\n",
    "An embedding layer is learned jointly with a neural network model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "\n",
    "EMBEDDING_DIM = HIDDEN_DIM = 128\n",
    "\n",
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                            embedding_dim=EMBEDDING_DIM)\n",
    "\n",
    "# BasicTextFieldEmbedder for tokens, not for (unchanged) labels\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Model\n",
    "\n",
    "The model we create with AllenNLP is in many ways the same as we might with plain PyTorch.  But a number of things have been usefully abstracted for us as well.  This model is most useful using a recurrent neural network (such as LSTM) for its `encoder`, but it abstracts from the specific network type with the `Seq2VecEncoder` wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict   # AllenNLP makes wide use of type annotations\n",
    "import torch\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder\n",
    "from allennlp.nn.util import get_text_field_mask\n",
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder\n",
    "from allennlp.training.metrics import CategoricalAccuracy, F1Measure\n",
    "\n",
    "class Classifier(Model):\n",
    "    def __init__(self,\n",
    "                 word_embeddings: TextFieldEmbedder,\n",
    "                 encoder: Seq2VecEncoder,\n",
    "                 vocab: Vocabulary,\n",
    "                 layer: torch.nn.Module,\n",
    "                 positive_label: int = 4) -> None:\n",
    "        super().__init__(vocab)\n",
    "        # We need the embeddings to convert word IDs to their vector representations\n",
    "        self.word_embeddings = word_embeddings\n",
    "\n",
    "        # Seq2VecEncoder is a neural network abstraction that takes a sequence of something\n",
    "        # (usually a sequence of embedded word vectors), processes it, and returns it as a single\n",
    "        # vector. Oftentimes, this is an RNN-based architecture (e.g., LSTM or GRU), but\n",
    "        # AllenNLP also supports CNNs and other simple architectures (for example,\n",
    "        # just averaging over the input vectors).\n",
    "        self.encoder = encoder\n",
    "        # FIXME: hack to keep layer within encoder for GPU memory fixes\n",
    "        self.layer = layer\n",
    "\n",
    "        # After converting a sequence of vectors to a single vector, we feed it into\n",
    "        # a fully-connected linear layer to reduce the dimension to the total number of labels.\n",
    "        self.hidden2tag = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
    "                                          out_features=vocab.get_vocab_size('labels'))\n",
    "        \n",
    "        # Monitor the metrics - we use accuracy, as well as prec, rec, f1 for 4 (very positive)\n",
    "        self.f1 = F1Measure(positive_label)        \n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "\n",
    "        # We use the cross-entropy loss because this is a classification task.\n",
    "        # Note that PyTorch's CrossEntropyLoss combines softmax and log likelihood loss,\n",
    "        # which makes it unnecessary to add a separate softmax layer.\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Instances are fed to forward after batching.\n",
    "    # Fields are passed through arguments with the same name.\n",
    "    def forward(self,\n",
    "                tokens: Dict[str, torch.Tensor],\n",
    "                label: torch.Tensor = None) -> torch.Tensor:\n",
    "        # Some GPU memory bookkeeping\n",
    "        self.layer.flatten_parameters()\n",
    "        \n",
    "        # In deep NLP, when sequences of tensors in different lengths are batched together,\n",
    "        # shorter sequences get padded with zeros to make them of equal length.\n",
    "        # Masking is the process to ignore extra zeros added by padding\n",
    "        mask = get_text_field_mask(tokens)\n",
    "\n",
    "        # Forward pass\n",
    "        embeddings = self.word_embeddings(tokens)\n",
    "        encoder_out = self.encoder(embeddings, mask)\n",
    "        logits = self.hidden2tag(encoder_out)\n",
    "\n",
    "        # Returned output dictionary must contain a \"loss\" key for your model\n",
    "        output = {\"logits\": logits}\n",
    "        if label is not None:\n",
    "            self.accuracy(logits, label)\n",
    "            self.f1(logits, label)\n",
    "            output[\"loss\"] = self.loss_function(logits, label)\n",
    "        return output    \n",
    "    \n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        # Could add more reporting, e.g.:\n",
    "        # precision, recall, f1 = self.f1.get_metric(reset)\n",
    "        # return {'precision': precision, 'recall': recall, 'f1': f1}\n",
    "        return {'accuracy': self.accuracy.get_metric(reset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.seq2vec_encoders import PytorchSeq2VecWrapper\n",
    "\n",
    "layer = torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True)\n",
    "\n",
    "# Wrap layer in suitable| word2vec representation\n",
    "lstm = PytorchSeq2VecWrapper(layer)\n",
    "    \n",
    "model = Classifier(word_embeddings, lstm, vocab, layer)\n",
    "model = model.cuda(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serializing\n",
    "\n",
    "Models often take a long while to train.  It is good to save them for reuse later.  Here we load the trained model from disk, and we can decide whether to perform the training anew or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "RETRAIN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.training.trainer import Trainer\n",
    "\n",
    "if RETRAIN:\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    iterator = BucketIterator(batch_size=32, sorting_keys=[(\"tokens\", \"num_tokens\")])\n",
    "    iterator.index_with(vocab)\n",
    "\n",
    "    trainer = Trainer(model=model,\n",
    "                      optimizer=optimizer,\n",
    "                      iterator=iterator,\n",
    "                      train_dataset=train_dataset,\n",
    "                      validation_dataset=dev_dataset,\n",
    "                      patience=10,\n",
    "                      num_epochs=40,\n",
    "                      cuda_device=device)\n",
    "\n",
    "    summary = trainer.train()\n",
    "    model.summary = summary\n",
    "    pickle.dump(model, open('data/sentiment-model.pkl', 'wb'))\n",
    "else:\n",
    "    model = pickle.load(open('data/sentiment-model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model will display something like this, with progress bars advancing during training.\n",
    "\n",
    "<pre style=\"background-color:#FFDDDD\">\n",
    "accuracy: 0.7088, loss: 0.7621 ||: 100%|██████████| 9956/9956 [01:16&lt;00:00, 129.86it/s]\n",
    "accuracy: 0.7413, loss: 0.6492 ||: 100%|██████████| 1296/1296 [00:05&lt;00:00, 254.08it/s]\n",
    "accuracy: 0.7964, loss: 0.5178 ||: 100%|██████████| 9956/9956 [01:11&lt;00:00, 139.46it/s]\n",
    "accuracy: 0.7970, loss: 0.5181 ||: 100%|██████████| 1296/1296 [00:04&lt;00:00, 277.51it/s]\n",
    "[...]\n",
    "accuracy: 0.8835, loss: 0.2892 ||: 100%|██████████| 9956/9956 [01:13&lt;00:00, 134.84it/s]\n",
    "accuracy: 0.8024, loss: 0.5155 ||: 100%|██████████| 1296/1296 [00:04&lt;00:00, 279.73it/s]\n",
    "accuracy: 0.8853, loss: 0.2845 ||: 100%|██████████| 9956/9956 [01:11&lt;00:00, 139.16it/s]\n",
    "accuracy: 0.8023, loss: 0.5251 ||: 100%|██████████| 1296/1296 [00:04&lt;00:00, 296.28it/s]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in model.summary.items():\n",
    "    print(k.rjust(25), '|', v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions\n",
    "\n",
    "It is straightfoward to make predictions once we have a trained model.  We need to wrap the model itself in an actual predictor, such as the one [provided by Dr. Hagiwara](https://github.com/mhagiwara/realworldnlp) named `SentenceClassifierPredictor`.  But making a prediction follows the somewhat more intuitive scikit-learn style of calling a `.predict()` method rather than the `pytorch.nn` style of calling the model itself. \n",
    "\n",
    "At times the classification chosen is not always strongly univocal from the model, and in some cases two far apart options are of similar preference.  In the ideal case, one logit value would be strongly greater than all others, but that is not always the case. I.e. possibly slightly more training data or slightly different parameters might tip the scale between very different predictions.  The configuration we have programmed does a good job of keeping adjacency of first and second guesses for the sample we use though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from src.predictors import SentenceClassifierPredictor\n",
    "\n",
    "sentiments = {'0': \"Very negative\",\n",
    "              '1': \"Mildly negative\",\n",
    "              '2': \"Neutral\",\n",
    "              '3': \"Mildly positive\",\n",
    "              '4': \"Very positive\"}\n",
    "\n",
    "phrases = [\"This is the best movie ever!\",\n",
    "           \"This is the worst movie ever!\",\n",
    "           \"The part that is good is not original, the part that is original is not good.\",\n",
    "           \"A day that will live in infamy.\",\n",
    "           \"When one burns one's bridges, what a very nice fire it makes.\",\n",
    "           \"You will contract a rare disease.\",\n",
    "           \"The only people for me are the mad one.\",\n",
    "           \"Never give an inch!\",\n",
    "           \"This movie was actually neither that funny, nor super witty.\",\n",
    "          ]\n",
    "        \n",
    "for phrase in phrases:\n",
    "    predictor = SentenceClassifierPredictor(model, dataset_reader=reader)\n",
    "    logits = predictor.predict(phrase)['logits']\n",
    "    ranking = np.argsort(logits)\n",
    "    first = ranking[-1]\n",
    "    second = ranking[-2]\n",
    "\n",
    "    sentiment = model.vocab.get_token_from_index(first, 'labels')\n",
    "    sentiment2 = model.vocab.get_token_from_index(second, 'labels')\n",
    "    print(f'{logits[first]:5.1f} {sentiments[sentiment]:15s} | {phrase}',\n",
    "          f'\\n{logits[second]:5.1f} {sentiments[sentiment2]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting the network\n",
    "\n",
    "We can experiment with other network details easily enough using the scaffolding we have already created.  For example, perhaps we speeculate that a gated recurrent unit (GRU) will work better for the recurrent layer than a multi-layer long short-term memory (LSTM).  Moreover, we also want to try using RMSprop rather than Adam for the optimizer.  Plus we decrease the `patience` to cause a faster step-down in the learning rate.\n",
    "\n",
    "These particular changes are not particularly theory based (but they are not absurd either); the example here is simply to show how we can easily vary those design details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Same iterator\n",
    "iterator = BucketIterator(batch_size=32, sorting_keys=[(\"tokens\", \"num_tokens\")])\n",
    "iterator.index_with(vocab)\n",
    "\n",
    "# Different optimizer\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "# Different RNN layer\n",
    "layer = torch.nn.GRU(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True)\n",
    "gru = PytorchSeq2VecWrapper(layer)\n",
    "\n",
    "model = Classifier(word_embeddings, gru, vocab, layer).cuda(device)\n",
    "\n",
    "# Different patience for LR decay\n",
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=train_dataset,\n",
    "                  validation_dataset=dev_dataset,\n",
    "                  patience=5,\n",
    "                  num_epochs=20,\n",
    "                  cuda_device=device)\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
