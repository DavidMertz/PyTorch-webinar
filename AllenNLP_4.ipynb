{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing with AllenNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\">What is AllenNLP?</font>\n",
    "<a href=\"AllenNLP_0.ipynb\"><img src=\"img/open-notebook.png\" align=\"right\"/></a>\n",
    "\n",
    "<font size=\"+1\">What is SpaCy?</font>\n",
    "<a href=\"AllenNLP_1.ipynb\"><img src=\"img/open-notebook.png\" align=\"right\"/></a>\n",
    "\n",
    "<font size=\"+1\">High Level Interfaces to NLP using PyTorch</font>\n",
    "<a href=\"AllenNLP_2.ipynb\"><img src=\"img/open-notebook.png\" align=\"right\"/></a>\n",
    "\n",
    "<font size=\"+1\">Sentiment Analysis</font>\n",
    "<a href=\"AllenNLP_3.ipynb\"><img src=\"img/open-notebook.png\" align=\"right\"/></a>\n",
    "\n",
    "<font size=\"+1\"><b><u>Part-of-Speech Tagging</u></b></font> \n",
    "<a href=\"AllenNLP_4.ipynb\"><img src=\"img/open-notebook.png\" align=\"right\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Part-of-Speech Tagging\n",
    "\n",
    "This lesson is largely based on an [official tutorial](https://github.com/allenai/allennlp/tree/master/tutorials/tagger) in the AllenNLP GitHub repository, by [Joel Grus](https://joelgrus.com/). A similar but somewhat different version is on the [AllenNLP website](https://allennlp.org/tutorials) (I'm unsure about the author(s) of that one).  I borrow a few ideas from both of these, and both of them are fairly closely modeled on the PyTorch [parts-of-speech tutorial](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html#example-an-lstm-for-part-of-speech-tagging) that I present in an earlier chapter.\n",
    "\n",
    "There is actually *more* scaffolding here than in the basic PyTorch example. But it does more to allow configurability as well.  In the main, this model and setup is largely the same as we saw with Sentiment Analysis; the main difference is really simply the different tagging of the training data for this different purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a minor matter, a number of functions in AllenNLP echo progress messages to STDERR in a way I find distracting for these lessons. We can stash them in a log file instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import redirect_stderr\n",
    "log = open('stderr.log', 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also check for CUDA, which will make things run much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    device = 0\n",
    "    print(torch.cuda.get_device_name(device))\n",
    "    print(f\"GPU memory used: {torch.cuda.memory_allocated(device):,}\")\n",
    "else:\n",
    "    device = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Reader\n",
    "\n",
    "Although we only read a tiny toy example here, in concept we might use this reader to read in a real-world tagged corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AllenNLP typically uses type annotations widely\n",
    "# The following are needed to satisfy the type annotations of `PosDatasetReader`\n",
    "from typing import Iterator, List, Dict\n",
    "from allennlp.data.token_indexers import TokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.data import Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `._read()` method is a required method for subclasses of `DataSetReader`.  It yields a sequence of these slightly funny `Instance` objects (based on tokens and tags). The `.text_to_instance()` method is just a helper with no special significance to the `DataSetReader` parent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.data.fields import TextField, SequenceLabelField\n",
    "from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
    "\n",
    "class PosDatasetReader(DatasetReader):\n",
    "    \"\"\"DatasetReader for PoS tagging data\n",
    "    \n",
    "    One sentence per line, e.g.\n",
    "\n",
    "        The###DET dog###NN ate###V the###DET apple###NN\n",
    "    \"\"\"\n",
    "    def __init__(self, token_indexers: Dict[str, TokenIndexer] = None) -> None:\n",
    "        super().__init__(lazy=False)\n",
    "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "\n",
    "    def text_to_instance(self, tokens: List[Token], tags: List[str] = None) -> Instance:\n",
    "        sentence_field = TextField(tokens, self.token_indexers)\n",
    "        fields = {\"sentence\": sentence_field}\n",
    "        \n",
    "        if tags:\n",
    "            label_field = SequenceLabelField(labels=tags, sequence_field=sentence_field)\n",
    "            fields[\"labels\"] = label_field\n",
    "\n",
    "        return Instance(fields)\n",
    "\n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "        with open(file_path) as f:\n",
    "            for line in f:\n",
    "                pairs = line.strip().split()\n",
    "                sentence, tags = zip(*(pair.split(\"###\") for pair in pairs))\n",
    "                yield self.text_to_instance([Token(word) for word in sentence], tags)\n",
    "\n",
    "reader = PosDatasetReader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the training and validation datasets\n",
    "\n",
    "In this case, the training is two tagged sentences, and the validation is one more sentence.  We could point to other URLs for non-toy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo Training\n",
    "!curl https://raw.githubusercontent.com/allenai/allennlp/master/tutorials/tagger/training.txt\n",
    "    \n",
    "!echo \"\\nValidation\"\n",
    "!curl https://raw.githubusercontent.com/allenai/allennlp/master/tutorials/tagger/validation.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much of what AllenNLP provides as utility functions are similar to those in many other libraries.  For example, `cached_path()` simply downloads URLs but caches contents; there is nothing NLP or ML specific about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.common.file_utils import cached_path\n",
    "\n",
    "with redirect_stderr(log):\n",
    "    train_dataset = reader.read(cached_path(\n",
    "        'https://raw.githubusercontent.com/allenai/allennlp'\n",
    "        '/master/tutorials/tagger/training.txt'))\n",
    "    \n",
    "    validation_dataset = reader.read(cached_path(\n",
    "        'https://raw.githubusercontent.com/allenai/allennlp'\n",
    "        '/master/tutorials/tagger/validation.txt'))\n",
    "\n",
    "    # Vocabulary maps {token -> id} (and reverse mapping).\n",
    "    vocab = Vocabulary.from_instances(train_dataset + validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder\n",
    "from allennlp.modules.seq2seq_encoders import Seq2SeqEncoder\n",
    "from allennlp.nn.util import sequence_cross_entropy_with_logits\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "\n",
    "class LstmTagger(Model):\n",
    "    def __init__(self,\n",
    "                 word_embeddings: TextFieldEmbedder,\n",
    "                 encoder: Seq2SeqEncoder,\n",
    "                 vocab: Vocabulary) -> None:\n",
    "        # Pass the vocab to the base class constructor.\n",
    "        super().__init__(vocab)\n",
    "        \n",
    "        # Store embedding and encoder\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.encoder = encoder\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        # Examine encoder to find input dimension, vocab to find output dimension\n",
    "        self.hidden2tag = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
    "                                          out_features=vocab.get_vocab_size('labels'))\n",
    "        \n",
    "        # Categorical accuracy metric\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "\n",
    "    def forward(self,\n",
    "                sentence: Dict[str, torch.Tensor],\n",
    "                labels: torch.Tensor = None) -> Dict[str, torch.Tensor]:\n",
    "        # AllenNLP pads the shorter inputs so that batch has uniform shape,\n",
    "        # ...use a mask to exclude the padding.\n",
    "        # `get_text_field_mask()` returns a tensor of 0s and 1s for padded and unpadded\n",
    "        mask = get_text_field_mask(sentence)\n",
    "        \n",
    "        # Convert sentence as a sequence of token ids into a sequence of embedded tensors.\n",
    "        embeddings = self.word_embeddings(sentence)\n",
    "        \n",
    "        # Pass embedded tensors and mask to the LSTM\n",
    "        encoder_out = self.encoder(embeddings, mask)\n",
    "\n",
    "        # Pass each encoded output tensor to feedforward layer to produce logits \n",
    "        tag_logits = self.hidden2tag(encoder_out)\n",
    "        output = {\"tag_logits\": tag_logits}\n",
    "\n",
    "        # Calculate the loss\n",
    "        # NOTE: Important to skip accuracy check if no labels passed...\n",
    "        #   this is situation when making a prediction, will crash \n",
    "        #   if attempted without this check\n",
    "        if labels is not None:\n",
    "            self.accuracy(tag_logits, labels, mask)\n",
    "            output[\"loss\"] = sequence_cross_entropy_with_logits(tag_logits, labels, mask)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {\"accuracy\": self.accuracy.get_metric(reset)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.seq2seq_encoders import PytorchSeq2SeqWrapper\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6\n",
    "\n",
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                            embedding_dim=EMBEDDING_DIM)\n",
    "\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "\n",
    "lstm = PytorchSeq2SeqWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\n",
    "\n",
    "model = LstmTagger(word_embeddings, lstm, vocab)\n",
    "model = model.cuda(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "We need to choose an optimizer for our model, in this case SGD (Stochastic gradient descent optimizer).  We create an iterator using the common `BucketIterator` from AllenNLP.  There is a bit to setup here, but it is also very boilerplate, and very similar to the last lesson.\n",
    "\n",
    "One thing we will utilize here is a learning rate scheduler.  There many of these in PyTorch itself, AllenNLP wraps them slightly for its own APIs.  We had not used such a utility class in the training loops of prior lessons, but rather constructed them more manually in our loops.  In this case, we pass all the work to the parameterized `Trainer` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from allennlp.data.iterators import BasicIterator\n",
    "from allennlp.common.params import Params\n",
    "from allennlp.training.learning_rate_schedulers import LearningRateScheduler\n",
    "from allennlp.training.trainer import Trainer\n",
    "from allennlp.nn.util import get_text_field_mask\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "iterator = BasicIterator(batch_size=2)\n",
    "iterator.index_with(vocab)\n",
    "\n",
    "params = Params({\"type\": \"reduce_on_plateau\"})\n",
    "scheduler = LearningRateScheduler.from_params(optimizer, params)\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  learning_rate_scheduler=scheduler,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=train_dataset,\n",
    "                  validation_dataset=validation_dataset,\n",
    "                  patience=10,\n",
    "                  num_epochs=1000,\n",
    "                  cuda_device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "RETRAIN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN:\n",
    "    # During training, the loss should go down and the accuracy up\n",
    "    with redirect_stderr(log):\n",
    "        summary = trainer.train()\n",
    "        \n",
    "    model.summary = summary\n",
    "    pickle.dump(model, open('data/parts-of-speech-model.pkl', 'wb'))\n",
    "else:\n",
    "    model = pickle.load(open('data/parts-of-speech-model.pkl', 'rb'))\n",
    "    \n",
    "for k, v in model.summary.items():\n",
    "    print(k.rjust(25), '|', v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model will display something like this, with progress bars advancing during training (almost all lines omitted here, but those shown are in order):\n",
    "\n",
    "<pre style=\"background-color:#FFDDDD\">\n",
    "accuracy: 0.3333, loss: 1.1268 ||: 100%|██████████| 1/1 [00:00&lt;00:00, 406.70it/s]\n",
    "accuracy: 0.4444, loss: 1.1165 ||: 100%|██████████| 1/1 [00:00&lt;00:00, 102.56it/s]\n",
    "accuracy: 0.4444, loss: 1.1051 ||: 100%|██████████| 1/1 [00:00&lt;00:00, 347.99it/s]\n",
    "accuracy: 0.6667, loss: 0.7536 ||: 100%|██████████| 1/1 [00:00&lt;00:00, 147.59it/s]\n",
    "accuracy: 0.8889, loss: 0.6482 ||: 100%|██████████| 1/1 [00:00&lt;00:00, 102.44it/s]\n",
    "accuracy: 0.8889, loss: 0.5342 ||: 100%|██████████| 1/1 [00:00&lt;00:00, 103.33it/s]\n",
    "accuracy: 1.0000, loss: 0.0727 ||: 100%|██████████| 1/1 [00:00&lt;00:00, 118.90it/s]\n",
    "accuracy: 1.0000, loss: 0.0316 ||: 100%|██████████| 1/1 [00:00&lt;00:00, 106.72it/s]\n",
    "accuracy: 1.0000, loss: 0.0294 ||: 100%|██████████| 1/1 [00:00&lt;00:00, 90.12it/s]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from allennlp.predictors import SentenceTaggerPredictor\n",
    "\n",
    "predictor = SentenceTaggerPredictor(model, dataset_reader=reader)\n",
    "sentence = \"Everybody ate the apple\"\n",
    "tag_logits = predictor.predict(sentence)['tag_logits']\n",
    "\n",
    "tag_ids = np.argmax(tag_logits, axis=-1)\n",
    "\n",
    "parts = [model.vocab.get_token_from_index(i, 'labels') for i in tag_ids]\n",
    "print(\" \".join(f\"{word}###{part}\" \n",
    "               for word, part in zip(sentence.split(), parts)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
