{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhancing an Image Classifier \n",
    "\n",
    "* Starting with `torchvision.models`\n",
    "* Retraining pretrained models\n",
    "* Modifying Network Layers\n",
    "* Understanding Effects of Network Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning\n",
    "\n",
    "An important concept and tool to use once you have some basic idea of constructing neural networks is *transfer learning*.  Many of the things that a particular model learns can be applied to other problems that are somewhat similar in domain.  For example—perhaps the most used example of this—a great deal of image recognition amounts to be able to recognize object borders, general lighting conditions, overall shading region identification, and other overall image features.  Even if a model happens to be trained to recognize, e.g. houses versus kittens, much of the same pattern recognition would be helpful in distinguishing trucks from elephants.  Especially since training complex models can be extremely slow, and also simply because other people have done a great deal of work in finding good tagged image collections, it would be nice to re-use much of that work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credit\n",
    "\n",
    "As with several of the other lessons, I utilize and graciously thank other authors who have provided wondeful examples.  In this case, I found a blog post and code by Gilbert Adjei titled [Transfer Learning with PyTorch](https://heartbeat.fritz.ai/transfer-learning-with-pytorch-cfcb69016c72).  Looking through a fairly large number of similar writeups, I found his the most accessible and manageable of these.  I make some small modifications, but the credit for the code goes primarily to Adjei.\n",
    "\n",
    "What Adjei looks at is using the pretrained `densenet121` model that is one of numerous image models included with PyTorch, and adapting it to identify images of cells that either are or are not parasitized by malaria.  A nice overview of densenet design is contained in the post [The Efficiency of Densenet](https://medium.com/@smallfishbigsea/densenet-2b0889854a92), by Hao Gao.  For this purpose, and of the image models in PyTorch would be similar to work with.  As an exercise, it would be good to try to apply the techniques in this lesson to other provided models, such as AlexNet, VGG, ResNet, SqueezeNet, Inception v3, GoogLeNet (most of those themselves come in a number of variations with differing layers, depths, and other design distinctions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started\n",
    "\n",
    "This lesson will take about 30-45 minutes to run, even with a fast GPU.  Probably many hours for CPU-only.  For those following along, it is a good idea to \"Run All Cells\" before we begin talking about the concepts.  \n",
    "\n",
    "We need a fairly large dataset to run this model.  The collection of images contains about 28k of them, evenly divided between \"Parasitized\" and \"Uninfected\" classes.  The archive of all the images is a bit over 300 MiB.  I have hosted it on my personal website for convenience.  The underlying dataset comes from the US [National Institutes of Health](https://ceb.nlm.nih.gov/repositories/malaria-datasets/).  In some basic testing, my web host seems to be faster than any of the NIH website, the Kaggle site where it is mirrored, or the Google Drive location where Adjei uploaded it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us download the dataset we need for this lesson\n",
    "from os.path import exists\n",
    "from os import remove\n",
    "from urllib.request import urlretrieve\n",
    "from zipfile import ZipFile\n",
    "\n",
    "if not exists('data/cell_images/Parasitized') or\\\n",
    "   not exists('data/cell_images/Uninfected'):\n",
    "    fname, header = urlretrieve('http://gnosis.cx/download/cell_images.zip')\n",
    "\n",
    "    with ZipFile(fname) as zf:\n",
    "        zf.extractall('data/')\n",
    "    remove(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/cell_images/\n",
      "├── Parasitized\n",
      "│   ├── C100P61ThinF_IMG_20150918_144104_cell_162.png\n",
      "│   ├── C100P61ThinF_IMG_20150918_144104_cell_163.png\n",
      "--\n",
      "--\n",
      "│   ├── C99P60ThinF_IMG_20150918_142334_cell_9.png\n",
      "│   └── Thumbs.db\n",
      "└── Uninfected\n",
      "    ├── C100P61ThinF_IMG_20150918_144104_cell_128.png\n",
      "    ├── C100P61ThinF_IMG_20150918_144104_cell_131.png\n"
     ]
    }
   ],
   "source": [
    "!tree data/cell_images/ | grep --context=2 '\\(Parasitized\\|Uninfected\\)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, let us put all our imports together at the start, as we would in a standalone script.  In some notebooks I have interspersed the imports to emphasize what code they pertain to, but there are few enough here that the start is a better place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from glob import glob\n",
    "from random import randrange\n",
    "import os\n",
    "from time import time\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, let us try to use CUDA GPU if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA memory used: 0\n"
     ]
    }
   ],
   "source": [
    "# For demonstration, we can use CPU target if CUDA not available\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Check the status of the GPU (if present)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA memory used:\", torch.cuda.memory_allocated())\n",
    "    device = torch.device('cuda') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A sample image\n",
    "\n",
    "Selected more-or-less at random, we can look at one typical image from the dataset to get a sense of what a cell image looks like.  The dimensions of the images are not completely uniform, but they are all of similar size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115, 115, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fname = 'img/cell-image.png'\n",
    "image = Image.open(fname)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "    \n",
    "np.array(image).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing images as inputs\n",
    "\n",
    "As we usually do when working with image data, we wish to perform a pipeline of image transforms on our raw images to give them all the same tensor size that we wish to work with.  This may involve sizing, cropping, color adjustment, and other normalization of the images.\n",
    "\n",
    "In the case of the training data, we add some additonal transforms to the pipeline.  In order to give the model a more robust set of training images, we randomly resize, flip, rotate, and add color jitter to the input images.  This should make the model capable of recognizing images that suffer from similar randomness in their original acquisition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your transforms for the training, validation, and testing sets\n",
    "jitter_image = transforms.Compose([transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "                                   transforms.RandomRotation(degrees=15),\n",
    "                                   transforms.ColorJitter(),\n",
    "                                   transforms.RandomHorizontalFlip(),\n",
    "                                   transforms.CenterCrop(size=224),  # Image net standards\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                        [0.229, 0.224, 0.225])\n",
    "                                   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_image = transforms.Compose([transforms.Resize(256),\n",
    "                                  transforms.CenterCrop(224),\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                       [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset\n",
    "\n",
    "Our dataset is based around the directory structure of our samples. Each of the subdirectories, `Parasitized/` and `Uninfected/` in this case, correspond to one class in the target. We further split up the dataset into three sampler for training, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 19,291; Validation: 5,511; Testing: 2,756\n"
     ]
    }
   ],
   "source": [
    "# Load from subdirectories indicating classes\n",
    "train_data = datasets.ImageFolder('data/cell_images/', transform=jitter_image)\n",
    "\n",
    "# percentage of training set to use as validation & testing\n",
    "valid_size, test_size = 0.2, 0.1\n",
    "\n",
    "# Perform a train/test/validation split (sklearn train_test_split() might be more elegant)\n",
    "ntrain = len(train_data)\n",
    "indices = np.arange(ntrain)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "valid_split = int(valid_size * ntrain)\n",
    "test_split = int((valid_size+test_size) * ntrain)\n",
    "\n",
    "train_idx = indices[test_split:]\n",
    "valid_idx = indices[:valid_split]\n",
    "test_idx =  indices[valid_split:test_split]\n",
    "\n",
    "# Samplers for training, validation, and testing batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "test_sampler = SubsetRandomSampler(test_idx)\n",
    "\n",
    "# Data loaders (combine dataset and sampler)\n",
    "params = {'batch_size': 32, 'num_workers': os.cpu_count()}\n",
    "train_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, **params)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, sampler=valid_sampler, **params)\n",
    "test_loader = torch.utils.data.DataLoader(train_data, sampler=test_sampler, **params)\n",
    "\n",
    "print(f\"Training: {len(train_idx):,}; Validation: {len(valid_idx):,}; Testing: {len(test_idx):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load then enhance the pre-trained model\n",
    "\n",
    "By freezing the parameters in the trained model, and only adding extra layers, what we accomplish is turning the entire previous model into basically just a utility for feature engineering.  Densenet121 is just a fancy way of generating some high level synthetic features that we would like to use in **our** model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(m.weight.data)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DenseNet(\n",
       "  (features): Sequential(\n",
       "    (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu0): ReLU(inplace)\n",
       "    (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (denseblock1): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (transition1): _Transition(\n",
       "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (denseblock2): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer7): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer8): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer9): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer10): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer11): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer12): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (transition2): _Transition(\n",
       "      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (denseblock3): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer7): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer8): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer9): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer10): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer11): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer12): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer13): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer14): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer15): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer16): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer17): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer18): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer19): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer20): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer21): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer22): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer23): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer24): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (transition3): _Transition(\n",
       "      (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (denseblock4): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer7): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer8): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer9): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer10): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer11): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer12): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer13): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer14): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer15): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer16): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=1024, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.densenet121(pretrained=True)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.require_grad = False\n",
    "\n",
    "# Define our own network, layers after the existing model, but basically just treating\n",
    "# the pre-trained classifier as a feature engineering step\n",
    "classifier = nn.Sequential(\n",
    "                nn.Linear(1024, 460),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.4),\n",
    "                nn.Linear(460,2),\n",
    "                nn.LogSoftmax(dim=1)\n",
    "            )\n",
    "\n",
    "model.classifier = classifier.to(device)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# We want to only update the parameters of the classifier\n",
    "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the enhanced model\n",
    "\n",
    "In the training loop below, we do something that is good general practice.  After each epoch, we see whether that epoch has the best loss (or meets some other metric we are aiming for) and snapshot it only if it represents an improvement.  When we load the model later, we will be working with the best epoch.  Of course, we could also later try further training the model with more epochs and/or new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0\n",
      "Training: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:2016: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 2])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target and input must have the same number of elements. target nelement (32) != input nelement (64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-5cbe89704438>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mlogps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    502\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2017\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2018\u001b[0m         raise ValueError(\"Target and input must have the same number of elements. target nelement ({}) \"\n\u001b[0;32m-> 2019\u001b[0;31m                          \"!= input nelement ({})\".format(target.numel(), input.numel()))\n\u001b[0m\u001b[1;32m   2020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2021\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Target and input must have the same number of elements. target nelement (32) != input nelement (64)"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "valid_loss_min = np.Inf    # Initial best loss as infinite\n",
    "model_save_name = \"Malaria.pt\"\n",
    "path = f\"data/{model_save_name}\"\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"EPOCH\", epoch)\n",
    "    start = time()\n",
    "    \n",
    "    # Notify the model we are in training mode\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    print(\"Training: \", end='')\n",
    "    for n, (inputs, labels) in enumerate(train_loader):\n",
    "        # Move input and label tensors to the default device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logps = model(inputs)\n",
    "        loss = criterion(logps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        if not n % 20:\n",
    "            print('.', end='', flush=True)\n",
    "    print(n, \"loops\")\n",
    "           \n",
    "    # Notify the model we are in eval mode (but also explicitly no_grad() for the work)\n",
    "    model.eval()\n",
    "    print(\"Validating: \", end='')\n",
    "    with torch.no_grad():\n",
    "        accuracy = 0\n",
    "        for n, (inputs, labels) in enumerate(valid_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            logps = model.forward(inputs)\n",
    "            batch_loss = criterion(logps, labels)\n",
    "            valid_loss += batch_loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            ps = torch.exp(logps)\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "            equals = top_class == labels.view(*top_class.shape)\n",
    "            accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "            if not n % 10:\n",
    "                print('.', end='', flush=True)\n",
    "        print(n, \"loops\")\n",
    "                    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    valid_loss = valid_loss/len(valid_loader)\n",
    "    valid_accuracy = accuracy/len(valid_loader) \n",
    "      \n",
    "    # print training/validation statistics \n",
    "    print(f'Training Loss: {train_loss:.3f}')\n",
    "    print(f'Validation loss: {valid_loss_min:.3f} ⟶ {valid_loss:.3f}')\n",
    "    print(f'Validation Accuracy: {valid_accuracy:.3f}')\n",
    "\n",
    "    # Only save the model if it is an improvement from best epoch\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print(f'Saving model...')\n",
    "        torch.save(model.state_dict(), path)\n",
    "        valid_loss_min = valid_loss\n",
    "    else:\n",
    "        print(f'Not saving because not best epoch')\n",
    "       \n",
    "    print(f\"Time for epoch: {int(time() - start)} seconds\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and visualizing results\n",
    "\n",
    "Here we use the test set that was held entirely separate from the training.  The validation was performed with `torch.no_grad()` in effect, but conceivably that dataset could have some systematic bias that produced inaccurate feedback into the loss function.  By testing against an entirely independent dataset, we get a good sanity check about our accuracy.\n",
    "\n",
    "In practice, if we do a reasonable train/test split on a reasonaly large dataset, there should be no important different between the validation and testing datasets, and we might often simply reuse the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, criterion):\n",
    "# monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        # move to GPU\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average test loss \n",
    "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "        # convert output probabilities to predicted class\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        # compare predictions to true label\n",
    "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "            \n",
    "    print('Test Loss: {:.6f}'.format(test_loss))\n",
    "    print('Test Accuracy: %2d%% (%2d/%2d)' % (\n",
    "                100. * correct / total, correct, total))\n",
    "\n",
    "test(model, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize a few categorized images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_input_image(img_path):    \n",
    "    image = Image.open(img_path)\n",
    "    # discard the transparent, alpha channel (that's the :3) and add the batch dimension\n",
    "    image = clean_image(image)[:3,:,:].unsqueeze(0)\n",
    "    return image\n",
    "\n",
    "def predict_malaria(model, class_names, img_path):\n",
    "    # load the image and return the predicted class\n",
    "    img = load_input_image(img_path)\n",
    "    model = model.cpu()\n",
    "    model.eval()\n",
    "    idx = torch.argmax(model(img))\n",
    "    return class_names[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names=['Parasitized','Uninfected']\n",
    "inf = np.array(glob(\"data/cell_images/Parasitized/*\"))\n",
    "uninf = np.array(glob(\"data/cell_images/Uninfected/*\"))\n",
    "\n",
    "for _ in range(2):\n",
    "    i = randrange(len(inf))\n",
    "    fname = inf[i]\n",
    "    img = Image.open(fname)\n",
    "    if predict_malaria(model, class_names, fname) == 'Parasitized':\n",
    "        print(f'[{i}] Diagnosed Parasitized (CORRECT)')\n",
    "    else:\n",
    "        print(f'[{i}] Diagnosed Uninfected (MISTAKE)')\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "for _ in range(2):\n",
    "    i = randrange(len(uninf))\n",
    "    fname = uninf[i]\n",
    "    img = Image.open(fname)\n",
    "    if predict_malaria(model, class_names, fname) == 'Uninfected':\n",
    "        print(f'[{i}] Diagnosed Uninfected (CORRECT)')\n",
    "    else:\n",
    "        print(f'[{i}] Diagnosed Parasitized (MISTAKE)')       \n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Lesson\n",
    "\n",
    "We have looked at quite a few capabilities in PyTorch and associated tools.  The next step is to go out and use these lovely tools in your own projects.  I'd love to hear back on what you find and see what you create.  Contact me via the repository for this training material (file issues, email me, propose PRs, whatever).\n",
    "\n",
    "Thanks. David Mertz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
