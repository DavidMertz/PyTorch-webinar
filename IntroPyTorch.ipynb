{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding PyTorch\n",
    "\n",
    "* Tensors and NumPy interfaces\n",
    "* Autograd\n",
    "* Using GPUs with `torch.cuda`\n",
    "* Parallelizing on clusters with `torch.distributed`\n",
    "* Create a neural network with `torch.nn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors and NumPy interfaces\n",
    "\n",
    "At a first pass, PyTorch tensors are very similar to NumPy arrays.  Both are ways of storing multi-dimensional data efficiently, and much of the same \"vectorized\" style of operation applies to both. Broadcasting and elementwise operations are similar.  Moreover, many of the same functions and methods exist in both PyTorch and NumPy, and conversion between the two formats is made straightforward by PyTorch.\n",
    "\n",
    "Where PyTorch tensors go beyond NumPy arrays, and are needed for neural networks are in a couple key areas.  As a not-so-minor matter, tensors can work transparently on GPUs as well as CPUs, and this can often vastly speed up operations.  NumPy does not build in that capability, but a number of projects allow this particular capability to be used outside of PyTorch, in varying ways (see [PyCUDA](https://documen.tician.de/pycuda/array.html), [Numba](https://numba.pydata.org/numba-doc/dev/cuda/index.html), [CuPy](https://cupy.chainer.org/), [MXNet](https://mxnet.incubator.apache.org/versions/master/tutorials/basic/ndarray.html), and probably others).\n",
    "\n",
    "You *could* use PyTorch simply to work with array computation on GPUs, but what is more likely to bring you here is an equally essential capability that is not present in those other mentioned libraries (by design): Autograd.  By storing the gradients from every operation (where autograd is enabled for any component tensors), PyTorch provides reverse automatic differentiation.  That is to say, it gives you a directed acyclic graph whose leaves are the input tensors and roots are the output tensors.  We explain this more below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that a derivative is only the *slope* of a curve, not its offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "x = torch.randn(3, 3, requires_grad=True)\n",
    "print(\"Random 2-D tensor\")\n",
    "print(x)\n",
    "\n",
    "# Perform an operation on tensor\n",
    "y = (x + 7) * 5\n",
    "print(\"\\nRandom 2-D shifted by 7, multiplied by 5\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6],\n",
    "                  [7, 8, 9]], dtype=torch.float)\n",
    "\n",
    "# grad_fn is derivative, so offset does not matter to slope\n",
    "y.grad_fn(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient need not be—and generally is not—a single scalar.  Each element of the tensor has its own gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = torch.randn(3, 3, requires_grad=True)\n",
    "print(\"Random 2-D tensor\")\n",
    "print(x2)\n",
    "\n",
    "# Perform an operation on tensor\n",
    "mul = torch.tensor([[2, 2, 2], [3, 3, 3], [5, 5, 5]], dtype=torch.float)\n",
    "y2 = x2 * mul\n",
    "print(\"\\nRandom 2-D  multiplied by 2, 3, or 5\")\n",
    "print(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's apply the various slopes to the tensor v\n",
    "y2.grad_fn(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform an additional vectorized operation, then a reduction\n",
    "z = y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(\"Multiplied by 3\")\n",
    "print(z)\n",
    "print(\"\\nMean of values\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = out.grad_fn\n",
    "indent = 1\n",
    "while True:\n",
    "    print(\" \"*indent, \"-->\", grad)\n",
    "    if not grad.next_functions:\n",
    "        break\n",
    "    grad = grad.next_functions[0][0]\n",
    "    indent += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The semantics of reverse operations are, to be honest, unintuitive in many cases.  \n",
    "\n",
    "So, for example, the gradient function attached to `out` after the `.mean()` operation is actually just distributing the values over an array of the input shape, but not scaling them to the same proportion as the starting values.  However, the operation *does* remember the input shape used to construct the reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out.grad_fn)\n",
    "\n",
    "val1 = Variable(torch.tensor([10.]))\n",
    "out.grad_fn(val1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val2 = torch.tensor([[3, 6, 9],\n",
    "                     [1, 2, 3],\n",
    "                     [2, 4, 8]], dtype=torch.float)\n",
    "out.grad_fn(val2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of this gradient function really does have to be 3x3; it's not simply a division by 9, nor a division by an arbitrary size of a tensor.  But because of broadcasting rules—the same as in NumPy, for those familiar with that library—shapes like a scalar or a less-dimensional shape can be broadcast to the target shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D of compatible length can be broadcase\n",
    "val3 = torch.tensor([1.5, 3.33, 9.1])\n",
    "out.grad_fn(val3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But not this\n",
    "try:\n",
    "    val4 = torch.tensor([[1., 2.],\n",
    "                         [3., 4.]])\n",
    "    out.grad_fn(val4)\n",
    "except Exception as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For mean in particular, we could easily enough redistribute the magnitude along with the distribution.  But there is no general \"go back to the last tensor exactly\" function that I know of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original tensor\\n\", val2, '\\n')\n",
    "\n",
    "size = np.prod(val2.shape)\n",
    "print(\"MeanBackward then rescaled manually\\n\", out.grad_fn(val2) * size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A low-level neural network\n",
    "\n",
    "We do not actually *need* autograd—nor the `torch.nn` package—to implement a neural network.  In fact, we can do it pretty easily in NumPy also.  These more advanced capabilities just make things easier.  The next several examples are lightly adapted from [Justin Johnson's discussion](https://github.com/jcjohnson/pytorch-examples), linked in the resources.  A very similar example appears in numerous places, so I am not sure of the first source of it.\n",
    "\n",
    "For this task, we will train a two-layer neural network to learn the relationship between 1000 random inputs and 10 random outputs.  In a first version, PyTorch looks almost the same as NumPy. First setup the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below illustration is of a simpler network than the one we actually create. In the image, a network with a 45 node input layer a 15 node hidden layer, and a 5 node output layer is shown just not to overwhelm as much visually.  Our 1000/100/10 is still a rather simple network by standards of most used in practice, nonetheless.\n",
    "\n",
    "In the image, a range of colors along the edges show possible weights of the connections.  What is displayed is simply random values, but a real network would be trained to have weights specific to the data and the problem at hand.  Somewhat subtly, the image also shows a bias input to the output layer.  Using this is a good idea, and is not conceptually difficult to implement at a low-level in code like the below, but we omit that for simplicity.\n",
    "\n",
    "<img src=\"img/random-perceptron.png\"/>\n",
    "\n",
    "Drawn with [NN-SVG](http://alexlenail.me/NN-SVG/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# N is training set size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N = 64\n",
    "D_in = 1000\n",
    "H = 100\n",
    "D_out = 10\n",
    "epochs = 501\n",
    "\n",
    "# The input and output tensors\n",
    "## The `requires_grad` argument relevant to 2nd example, harmless for 1st\n",
    "x = torch.randn(N, D_in, dtype=torch.float, requires_grad=False)\n",
    "y = torch.randn(N, D_out, dtype=torch.float, requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without autograd\n",
    "\n",
    "The basic array/tensor version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Expect ~2 min\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H)   # E.g. 1000 x 100 weights\n",
    "w2 = torch.randn(H, D_out)  # E.g. 100 x 10 weight\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(epochs):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.mm(w1)             # Spelled `x.dot(w1)` or `x @ w1` in NumPy\n",
    "    h_relu = h.clamp(min=0)  # Activation function\n",
    "    y_predict = h_relu.mm(w2)\n",
    "\n",
    "    # Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor;\n",
    "    # we can get its single value as a Python number with loss.item().\n",
    "    loss = (y_predict - y).pow(2).sum()  # mean squared error\n",
    "    if not t % 50:\n",
    "        print(\"Iteration: %4d - Loss: %0.2e\" % (t, loss.item()))\n",
    "        \n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    ## Notice that name `loss` is not directly used in backprop\n",
    "    grad_y_predict = 2.0 * (y_predict - y)\n",
    "    w2_grad = h_relu.t().mm(grad_y_predict)\n",
    "    grad_h_relu = grad_y_predict.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0        # Equivalent to `clamp(min=0)`\n",
    "    w1_grad = x.t().mm(grad_h)\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * w1_grad\n",
    "    w2 -= learning_rate * w2_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using autograd\n",
    "\n",
    "The above example works fine, and is not too difficult.  But we *did* need to perform the backward propagation on each \"layer\" (tensor of weights) manually.  For tens or hundreds of layers this gets awkward and more error prone.  By utilizing autograd, we automatically define a computational graph, and can back propagate a loss function automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Expect ~2 min\n",
    "\n",
    "w1 = torch.randn(D_in, H, dtype=torch.float, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, dtype=torch.float, requires_grad=True)\n",
    "\n",
    "for t in range(epochs):\n",
    "    # Forward pass: compute predicted y using operations on Tensors. Since w1 and\n",
    "    # w2 have requires_grad=True, operations involving these Tensors will cause\n",
    "    # PyTorch to build a computational graph, allowing automatic computation of\n",
    "    # gradients. Since we are no longer implementing the backward pass by hand we\n",
    "    # don't need to keep references to intermediate values.    \n",
    "    y_predict = x.mm(w1).clamp(min=0).mm(w2)   # <- ReLU in here\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_predict - y).pow(2).sum()\n",
    "    if not t % 50:\n",
    "        print(\"Iteration: %4d - Loss: %0.3e\" % (t, loss.item()))\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.    \n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent. For this step we just want to mutate\n",
    "    # the values of w1 and w2 in-place; we don't want to build up a computational\n",
    "    # graph for the update steps, so we use the torch.no_grad() context manager\n",
    "    # to prevent PyTorch from building a computational graph for the updates\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after running the backward pass\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a neural network with `torch.nn`\n",
    "\n",
    "Most of the time when you want to create a neural network, you will use the higher-level capabilities in `torch.nn`.  Technically, the lower level basic tensor operations, or also autograd, do not require you to design neural networks *per se*.  Any sort of abstract data flow through a computational graph is equally possible.  But the abstraction of arranging clusters neurons in layers, and passing calculation from layer to layer, is a widely used and powerful one.\n",
    "\n",
    "One difference worth noting between PyTorch and other popular neural network frameworks, is that PyTorch is fully dynamic.  In TensorFlow, you are required compile a model and use it as-is.  In PyTorch, the layers in a network are simply tensors that you can poke at and modify dynamically within your custom code.  Put another way, PyTorch uses eager execution whereas some other libraries lazily define a computational graph in full before they can be used.\n",
    "\n",
    "In practice, it is a relatively arcane technique to dynamically modify neural networks during training, and we will not cover that specifically in the beginning part of this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Expect ~2 min\n",
    "\n",
    "# These layers are equivalent to the weight tensors `w1` and `w2`\n",
    "# Note: nn.Sequential is a Module which contains other Modules, and \n",
    "# applies them in sequence to produce its output. Each Linear Module \n",
    "# contains internal Tensors for its weight and bias.\n",
    "model = torch.nn.Sequential(\n",
    "          torch.nn.Linear(D_in, H),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(H, D_out),\n",
    "        )\n",
    "\n",
    "# The nn package also contains definitions of numerous loss functions; \n",
    "# we use Mean Squared Error (MSE) here. Setting `reduction='sum'`  \n",
    "# computes the *sum* of squared errors rather than their mean; this\n",
    "# matches the prior examples, but it is more common to use mean\n",
    "# squared error as a loss by setting reduction='elementwise_mean'.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(epochs):\n",
    "    # Forward pass: compute predicted y by passing x to the model. Calling the \n",
    "    # model requires a Tensor of input data and produces a Tensor of output data.\n",
    "    y_predict = model(x)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the loss.\n",
    "    loss = loss_fn(y_predict, y)\n",
    "    if not t % 50:\n",
    "        print(\"Iteration: %4d - Loss: %0.3e\" % (t, loss.item()))\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its data and gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param.data -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to note.  You can get some basic information about a model simply by printing it or echoing it.  But I—and many other users—like a summary more similar to that Keras provides for other backends.  The external package `torchsummary` provides this, if you'd like it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    # torchsummary has a glitch. If a running on a CUDA-enabled build\n",
    "    # it only wants to print a CUDA model\n",
    "    summary(model, input_size=(1, D_in))\n",
    "else:\n",
    "    # Use this if first doesn't work\n",
    "    summary(model.to(torch.device('cuda')), input_size=(1, D_in))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on learning rate, bias, and initialization\n",
    "\n",
    "You may have noticed that we used a much higher learning rate for the built model than we did in the manual gradient cases.  There are a few crucial differences that make these examples not quite identical.  A `torch.nn.Linear` module builds in a few extra abilities that are desirable, but not shown in the simple manual examples.\n",
    "\n",
    "Of some importance, the initialization of weights is smarter in the linear layer than the simple random numbers in the manual case.  However, most of that difference is probably handled by a few  (dozen) rounds of the training.  More importantly, linear layers add a **bias** term automatically.  This is effectively the constant term in a linear function like `y = ax + b`; our simplest manual network lacks any such term which both makes it succeptible to overfitting and exploding loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can illustrate the relevance of bias to neuron activation with a few simple pictures.  The neural networks we build above used a ReLU activation fuction rather than sigmoid, but the concept and effect is similar in either case.  Let us see the possible responses with and without bias available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x, weight=1, bias=0):\n",
    "    return 1/(1+np.exp(-1 * (x * weight + bias)))\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16,5))\n",
    "x = np.linspace(-15, 15, 100)\n",
    "ax1.plot(x, sigmoid(x), x, sigmoid(x, 0.5), x, sigmoid(x, 3.0))\n",
    "ax1.set_title(\"Sigmoids under several weights\")\n",
    "ax2.plot(x, sigmoid(x), x, sigmoid(x, bias=-5), x, sigmoid(x, bias=5))\n",
    "ax2.set_title(\"Sigmoids under several biases\")\n",
    "ax3.plot(x, sigmoid(x),\n",
    "         x, sigmoid(x, 1.5, -6), \n",
    "         x, sigmoid(x, 0.5, -1), \n",
    "         x, sigmoid(x, 3, 6),\n",
    "         x, sigmoid(x, 0.75, -4),\n",
    "         x, sigmoid(x, 0.75, 7))\n",
    "ax3.set_title(\"Sigmoids varying both weight and bias\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GPUs with `torch.cuda`\n",
    "\n",
    "Targetting CUDA GPUs is almost trivial under torch.  The only change we need to make is indicating where the tensors live, and the rest of the code works just the same.  Well, at first brush.  Using multiple GPUs or manual memory management of GPUs, or other lower level concerns, require additional work.  But the easy thing is easy, and is likely to speed up fitting considerably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expect ~2 min\n",
    "from time import time\n",
    "\n",
    "for device in ['cuda', 'cpu']:\n",
    "    # For demonstration, we can use CPU target if CUDA not available\n",
    "    if device == 'cuda' and not torch.cuda.is_available():\n",
    "        continue\n",
    "    start = time()\n",
    "    print(\"Using\", device.upper())\n",
    "    device = torch.device(device)\n",
    "    \n",
    "    x = torch.randn(N, D_in, device=device)\n",
    "    y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "    # Could use any of the versions shown, but for example:\n",
    "    loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "    learning_rate = 1e-4\n",
    "    model = torch.nn.Sequential(\n",
    "              torch.nn.Linear(D_in, H),\n",
    "              torch.nn.ReLU(),\n",
    "              torch.nn.Linear(H, D_out),\n",
    "            ).to(device)\n",
    "\n",
    "    for t in range(epochs):\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in model.parameters():\n",
    "            param.data -= learning_rate * param.grad\n",
    "\n",
    "    print(\"Loss after training:\", loss.item())\n",
    "    print(\"Time elapsed: %.02f seconds\\n\" % (time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelizing on clusters with `torch.distributed`\n",
    "\n",
    "We cannot get into details in this course, but PyTorch includes capabilities to distribute training over a variety of different architectures, including multiple CPUs or multiple GPUs, including on clusters of machines.\n",
    "\n",
    "The tutorial at [Writing Distributed Applications with PyTorch](https://pytorch.org/tutorials/intermediate/dist_tuto.html) is a good place to get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Lesson\n",
    "\n",
    "**Tasks with Networks**: We just looked at building a few simple networks, in several styles, using PyTorch.  The next lessons delves into more complex and real world examples.\n",
    "\n",
    "<a href=\"NetworkExamples_0.ipynb\"><img src=\"img/open-notebook.png\" align=\"left\"/></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
