{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding PyTorch\n",
    "\n",
    "* Tensors and NumPy interfaces\n",
    "* Autograd\n",
    "* Using GPUs with `torch.cuda`\n",
    "* Parallelizing on clusters with `torch.distributed`\n",
    "* Create a neural network with `torch.nn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors and NumPy interfaces\n",
    "\n",
    "At a first pass, PyTorch tensors are very similar to NumPy arrays.  Both are ways of storing multi-dimensional data efficiently, and much of the same \"vectorized\" style of operation applies to both. Broadcasting and elementwise operations are similar.  Moreover, many of the same functions and methods exist in both PyTorch and NumPy, and conversion between the two formats is made straightforward by PyTorch.\n",
    "\n",
    "Where PyTorch tensors go beyond NumPy arrays, and are needed for neural networks are in a couple key areas.  As a not-so-minor matter, tensors can work transparently on GPUs as well as CPUs, and this can often vastly speed up operations.  NumPy does not build in that capability, but a number of projects allow this particular capability to be used outside of PyTorch, in varying ways (see [PyCUDA](https://documen.tician.de/pycuda/array.html), [Numba](https://numba.pydata.org/numba-doc/dev/cuda/index.html), [CuPy](https://cupy.chainer.org/), [MXNet](https://mxnet.incubator.apache.org/versions/master/tutorials/basic/ndarray.html), and probably others).\n",
    "\n",
    "You *could* use PyTorch simply to work with array computation on GPUs, but what is more likely to bring you here is an equally essential capability that is not present in those other mentioned libraries (by design): Autograd.  By storing the gradients from every operation (where autograd is enabled for any component tensors), PyTorch provides reverse automatic differentiation.  That is to say, it gives you a directed acyclic graph whose leaves are the input tensors and roots are the output tensors.  We explain this more below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that a derivative is only the *slope* of a curve, not its offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(3, 3, requires_grad=True)\n",
    "print(\"Random 2-D tensor\")\n",
    "print(x)\n",
    "\n",
    "# Perform an operation on tensor\n",
    "y = (x + 7) * 5\n",
    "print(\"\\nRandom 2-D shifted by 7, multiplied by 5\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6],\n",
    "                  [7, 8, 9]], dtype=torch.float)\n",
    "\n",
    "# grad_fn is derivative, so offset does not matter to slope\n",
    "y.grad_fn(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform an additional vectorized operation, then a reduction\n",
    "z = y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(\"Multiplied by 3\")\n",
    "print(z)\n",
    "print(\"\\nMean of values\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = out.grad_fn\n",
    "indent = 1\n",
    "while True:\n",
    "    print(\" \"*indent, \"-->\", grad)\n",
    "    if not grad.next_functions:\n",
    "        break\n",
    "    grad = grad.next_functions[0][0]\n",
    "    indent += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A low-level neural network\n",
    "\n",
    "We do not actually *need* autograd—nor the `torch.nn` package—to implement a neural network.  In fact, we can do it pretty easily in NumPy also.  These more advanced capabilities just make things easier.  The next several examples are lightly adapted from [Justin Johnson's discussion](https://github.com/jcjohnson/pytorch-examples), linked in the resources.  A very similar example appears in numerous places, so I am not sure of first source of it.\n",
    "\n",
    "For this task, we will train a two-layer neural network to learn the relationship between 1000 random inputs and 10 random outputs.  In a first version, PyTorch looks almost the same as NumPy. First setup the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# The input and output tensors\n",
    "## The `requires_grad` argument relevant to 2nd example, harmless for 1st\n",
    "x = torch.randn(N, D_in, dtype=torch.float, requires_grad=False)\n",
    "y = torch.randn(N, D_out, dtype=torch.float, requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without autograd\n",
    "\n",
    "The basic array/tensor version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H)\n",
    "w2 = torch.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(501):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.mm(w1)             # Spelled `x.dot(w1)` or `x @ w1` in NumPy\n",
    "    h_relu = h.clamp(min=0)  # Activation function\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor;\n",
    "    # we can get its single value as a Python number with loss.item().\n",
    "    loss = (y_pred - y).pow(2).sum()  # mean squared error\n",
    "    if not t % 50:\n",
    "        print(\"Iteration: %4d - Loss: %0.2e\" % (t, loss.item()))\n",
    "        \n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    ## Notice that name `loss` is not directly used in backprop\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0        # Equivalent to `clamp(min=0)`\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using autograd\n",
    "\n",
    "The above example works fine, and is not too difficult.  But we *did* need to perform the backward propagation on each \"layer\" (tensor of weights) manually.  For tens or hundreds of layers this gets awkward and more error prone.  By utilizing autograd, we automatically define a computational graph, and can back propagate a loss function automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = torch.randn(D_in, H, dtype=torch.float, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, dtype=torch.float, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(501):\n",
    "    # Forward pass: compute predicted y using operations on Tensors. Since w1 and\n",
    "    # w2 have requires_grad=True, operations involving these Tensors will cause\n",
    "    # PyTorch to build a computational graph, allowing automatic computation of\n",
    "    # gradients. Since we are no longer implementing the backward pass by hand we\n",
    "    # don't need to keep references to intermediate values.    \n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)   # <- ReLU in here\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if not t % 50:\n",
    "        print(\"Iteration: %4d - Loss: %0.3e\" % (t, loss.item()))\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.    \n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent. For this step we just want to mutate\n",
    "    # the values of w1 and w2 in-place; we don't want to build up a computational\n",
    "    # graph for the update steps, so we use the torch.no_grad() context manager\n",
    "    # to prevent PyTorch from building a computational graph for the updates\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after running the backward pass\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a neural network with `torch.nn`\n",
    "\n",
    "Most of the time when you want to create a neural network, you will use the higher-level capabilities in `torch.nn`.  Technically, the lower level basic tensor operations, or also autograd, do not require you to design neural networks *per se*.  Any sort of abstract data flow through a computational graph is equally possible.  But the abstraction of arranging clusters neurons in layers, and passing calculation from layer to layer, is a widely used and powerful one.\n",
    "\n",
    "One difference worth noting between PyTorch and other popular neural network frameworks, is that PyTorch is fully dynamic.  In TensorFlow, you are required compile a model and use it as-is.  In PyTorch, the layers in a network are simply tensors that you can poke at and modify dynamically within your custom code.  Put another way, PyTorch uses eager execution whereas some other libraries lazily define a computational graph in full before they can be used.\n",
    "\n",
    "In practice, it is a relatively arcane technique to dynamically modify neural networks during training, and we will not cover that specifically in the beginning part of this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# These layers are equivalent to the weight tensors `w1` and `w2`\n",
    "# Note: nn.Sequential is a Module which contains other Modules, and \n",
    "# applies them in sequence to produce its output. Each Linear Module \n",
    "# contains internal Tensors for its weight and bias.\n",
    "model = torch.nn.Sequential(\n",
    "          torch.nn.Linear(D_in, H),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(H, D_out),\n",
    "        )\n",
    "\n",
    "# The nn package also contains definitions of numerous loss functions; \n",
    "# we use Mean Squared Error (MSE) here. Setting `reduction='sum'`  \n",
    "# computes the *sum* of squared errors rather than their mean; this\n",
    "# matches the prior examples, but it is more common to use mean\n",
    "# squared error as a loss by setting reduction='elementwise_mean'.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(501):\n",
    "    # Forward pass: compute predicted y by passing x to the model. Calling the \n",
    "    # model requires a Tensor of input data and produces a Tensor of output data.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if not t % 50:\n",
    "        print(\"Iteration: %4d - Loss: %0.3e\" % (t, loss.item()))\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its data and gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param.data -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to note.  You can get some basic information about a model simply by printing it or echoing it.  But I—and many other users—like a summary more similar to that Keras provides for other backends.  The external package `torchsummary` provides this, if you'd like it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    # torchsummary has a glitch. If a running on a CUDA-enabled build\n",
    "    # it only wants to print a CUDA model\n",
    "    summary(model, input_size=(1, D_in))\n",
    "else:\n",
    "    # Use this if first doesn't work\n",
    "    summary(model.to(torch.device('cuda')), input_size=(1, D_in))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on learning rate, bias, and initialization\n",
    "\n",
    "You may have noticed that we used a much higher learning rate for the built model than we did in the manual gradient cases.  There are a few crucial differences that make these examples not quite identical.  A `torch.nn.Linear` module builds in a few extra abilities that are desirable, but not shown in the simple manual examples.\n",
    "\n",
    "Of some importance, the initialization of weights is smarter in the linear layer than the simple random numbers in the manual case.  However, most of that difference is probably handled by a few  (dozen) rounds of the training.  More importantly, linear layers add a **bias** term automatically.  This is effectively the constant term in a linear function like `y = ax + b`; our simplest manual network lacks any such term which both makes it succeptible to overfitting and exploding loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can illustrate the relevance of bias to neuron activation with a few simple pictures.  The neural networks we build above used a ReLU activation fuction rather than sigmoid, but the concept and effect is similar in either case.  Let us see the possible responses with and without bias available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x, weight=1, bias=0):\n",
    "    return 1/(1+np.exp(-1 * (x * weight + bias)))\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16,5))\n",
    "x = np.linspace(-15, 15, 100)\n",
    "ax1.plot(x, sigmoid(x), x, sigmoid(x, 0.5), x, sigmoid(x, 3.0))\n",
    "ax1.set_title(\"Sigmoids under several weights\")\n",
    "ax2.plot(x, sigmoid(x), x, sigmoid(x, bias=-5), x, sigmoid(x, bias=5))\n",
    "ax2.set_title(\"Sigmoids under several biases\")\n",
    "ax3.plot(x, sigmoid(x),\n",
    "         x, sigmoid(x, 1.5, -6), \n",
    "         x, sigmoid(x, 0.5, -1), \n",
    "         x, sigmoid(x, 3, 6),\n",
    "         x, sigmoid(x, 0.75, -4),\n",
    "         x, sigmoid(x, 0.75, 7))\n",
    "ax3.set_title(\"Sigmoids varying both weight and bias\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GPUs with `torch.cuda`\n",
    "\n",
    "Targetting CUDA GPUs is almost trivial under torch.  The only change we need to make is indicating where the tensors live, and the rest of the code works just the same.  Well, at first brush.  Using multiple GPUs or manual memory management of GPUs, or other lower level concerns, require additional work.  But the easy thing is easy, and is likely to speed up fitting considerably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# For demonstration, we can use CPU target if CUDA not available\n",
    "device = torch.device('cpu')\n",
    "# Uncomment this to run on GPU\n",
    "device = torch.device('cuda:0') \n",
    "\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Could use any of the versions shown, but for example:\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "learning_rate = 1e-4\n",
    "model = torch.nn.Sequential(\n",
    "          torch.nn.Linear(D_in, H),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(H, D_out),\n",
    "        ).to(device)\n",
    "\n",
    "for t in range(501):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in model.parameters():\n",
    "        param.data -= learning_rate * param.grad\n",
    "        \n",
    "print(\"Loss after training:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelizing on clusters with `torch.distributed`\n",
    "\n",
    "We cannot get into details in this course, but PyTorch includes capabilities to distribute training over a variety of different architectures, including multiple CPUs or multiple GPUs, including on clusters of machines.\n",
    "\n",
    "The tutorial at [Writing Distributed Applications with PyTorch](https://pytorch.org/tutorials/intermediate/dist_tuto.html) is a good place to get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Lesson\n",
    "\n",
    "**Tasks with Networks**: We just looked at building a few simple networks, in several styles, using PyTorch.  The next lessons delves into more complex and real world examples.\n",
    "\n",
    "<a href=\"NetworkExamples_0.ipynb\"><img src=\"img/open-notebook.png\" align=\"left\"/></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
