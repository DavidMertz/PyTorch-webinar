{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks with Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\">A simple feature classifier</font>\n",
    "<a href=\"NetworkExamples_0.ipynb\"><img src=\"img/open-notebook.png\" align=\"right\"/></a>\n",
    "\n",
    "<font size=\"+1\">An image classifier</font>\n",
    "<a href=\"NetworkExamples_1.ipynb\"><img src=\"img/open-notebook.png\" align=\"right\"/></a>\n",
    "\n",
    "<font size=\"+1\"><u><b>A regression prediction</b></u></font>\n",
    "<a href=\"NetworkExamples_2.ipynb\"><img src=\"img/open-notebook.png\" align=\"right\"/></a>\n",
    "\n",
    "<font size=\"+1\">Clustering with PyTorch</font>\n",
    "<a href=\"NetworkExamples_3.ipynb\"><img src=\"img/open-notebook.png\" align=\"right\"/></a>\n",
    "\n",
    "<font size=\"+1\">Generative Adversarial Networks (GAN)</font> \n",
    "<a href=\"NetworkExamples_4.ipynb\"><img src=\"img/open-notebook.png\" align=\"right\"/></a>\n",
    "\n",
    "<font size=\"+1\">Reinforcement Learning</font>\n",
    "<a href=\"NetworkExamples_5.ipynb\"><img src=\"img/open-notebook.png\" align=\"right\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For demonstration, we can use CPU target if CUDA not available\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Check the status of the GPU (if present)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.memory_allocated()\n",
    "    # *MUCH* faster to run on GPU\n",
    "    device = torch.device('cuda') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A regression prediction\n",
    "\n",
    "In the classification problem I took from my work about predicting garment sizes there was a simplification to try to help our DNN.  Actual sizes are two dimensional, having both a numeric size which measures \"width\" and a descriptor (\"LONG\", \"SHORT\", \"PETITE\" that measures length.  Of course, cloth is flexible, so these things interact somewhat.  In any case, the data provided only utilizes the main size componentâ€”a \"number\", although `00` is a special number that is different from `0` in garment sizing.\n",
    "\n",
    "Given only those single number sizes, we can actually put them in a linear order unambiguously.  Framing it that way suggests a regression problem rather than a classification problem.  In the rest of this notebook, we construct a model that is mostly the same as the classifcation we performed before, but is constructed as a regression instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/garments.csv.gz', dtype={'TARGET':str})\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the target\n",
    "\n",
    "For a regression problem, we do not wish to one-hot-encode the target.  Instead, we will simply convert the linearly ordered sizes to sequential integers.  These are, strictly speaking, ordinal rather than quantitative values, but it does not matter very much for this construction I.e. size `8` is not \"twice as much\" as size `4` in any measure, nor is it so in the integer sequence encoding; `8` is simply *more than* `4` to some degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['age', 'bra_size_chest', 'bra_size_cup', 'height', 'shoe_size', 'weight']]\n",
    "\n",
    "sizes = ['00', '0', '2', '4', '6', '8', '10', '12', '14', '16', '18']\n",
    "size_to_num = dict(zip(sizes, range(len(sizes))))\n",
    "num_to_size = {v:k for (k, v) in size_to_num.items()}\n",
    "\n",
    "y = df.TARGET.map(size_to_num)\n",
    "y.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only real difference in setting up the layer sizes is that the output dimension is one rather than a larger number of one-hot-encoded elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of input features\n",
    "in_dim = X.shape[1]\n",
    "\n",
    "# The number of \"polynomial features\" of order 2\n",
    "hidden1 = int(in_dim * 2 + (in_dim * (in_dim-1) / 2) + 1)\n",
    "out_dim = 1\n",
    "\n",
    "# The sizes of the \"inference layers\"/\n",
    "hidden2 = hidden3 = hidden4 = 2 * len(y.unique())  \n",
    "\n",
    "# Remind ourselves of the layer sizes\n",
    "in_dim, hidden1, hidden2, hidden3, hidden4, out_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing the training regime\n",
    "\n",
    "For the most part, the code in `do_training()` is the same as we used previously.  However, a lot of trial and error went into tweaking the learning rate decay to be \"pretty good\" for this problem.  Batches and epochs are trained *much* more quickly with this simplified target; but at the same time, it takes many more epochs for loss to reach a stable valley than it did with the classification version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_training(model, X_train, y_train, optimizer, loss_fn, \n",
    "                epochs=5000, batch_size=5000, early_stop=6):\n",
    "    \"Perform a training regime that includes automatic decay of learning rate\"\n",
    "    loss_history = []\n",
    "    print(\"+++ Beginning %d epochs with batch size %d\" % (epochs, batch_size))\n",
    "    for epoch in range(1, epochs+1):\n",
    "        for start in range(0, len(X_train), batch_size):\n",
    "            # Next batch of training rows\n",
    "            X = X_train[start:start+batch_size]\n",
    "            y = y_train[start:start+batch_size]\n",
    "\n",
    "            # Forward pass: compute predicted Y by passing X to the model.\n",
    "            y_pred = model(X)\n",
    "\n",
    "            # Compute loss.\n",
    "            loss = loss_fn(y_pred, y)\n",
    "                \n",
    "            # Before the backward pass, use the optimizer object to zero all of the\n",
    "            # gradients for the variables it will update (which are the learnable\n",
    "            # weights of the model). This is because by default, gradients are\n",
    "            # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "            # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Backward pass: compute gradient of the loss with respect to model\n",
    "            # parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # Calling the step function on an Optimizer makes an update to its\n",
    "            # parameters\n",
    "            optimizer.step()\n",
    "\n",
    "        # Print some progress information each epoch\n",
    "        print(\"Epoch %d; Loss: %0.6f (lr=%0.8f)\" % (\n",
    "               epoch, loss.item(), optimizer.param_groups[0]['lr']))\n",
    "        loss_history.append(loss.item())\n",
    "                    \n",
    "        # Is this regime currently failing to reduce loss?\n",
    "        ## Run for at least `early_stop` epochs\n",
    "        if len(loss_history) < early_stop:\n",
    "            continue\n",
    "            \n",
    "        ## Lower learning rate by 2x if no improvement in loss for multiple epochs\n",
    "        diff = max(loss_history[-early_stop:]) - min(loss_history[-early_stop:])\n",
    "        if  diff/loss_history[-1] < 0.005:\n",
    "            optimizer.param_groups[0]['lr'] /= 2\n",
    "            \n",
    "        ## If learning rate is lowered to tiny value, we are not getting anywhere\n",
    "        if optimizer.param_groups[0]['lr'] < 1e-8:           \n",
    "            print(\"+++ Discontinuing training regime when loss becomes constant\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model\n",
    "\n",
    "The only thing different in this model versus that used in the classifier is that the final layer has a single output, and there is no activation function applied to it.  This gives us a regression instead.  Perhaps not the optimal one possible, but at least framed the right way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sequential NN\n",
    "model = torch.nn.Sequential(\n",
    "    # This layer allows \"polynomial features\"\n",
    "    torch.nn.Linear(in_dim, hidden1),\n",
    "    # The activation is treated as a separate layer\n",
    "    torch.nn.ReLU(),\n",
    "\n",
    "    # This layer is \"inference\"\n",
    "    torch.nn.Linear(hidden1, hidden2),\n",
    "    torch.nn.ReLU(), \n",
    "    \n",
    "    # A Dropout layer sometimes reduces co-adaptation of neurons\n",
    "    torch.nn.Dropout(p=0.1),\n",
    "\n",
    "    # This layer is \"inference\"\n",
    "    torch.nn.Linear(hidden2, hidden3),\n",
    "    # Often Leaky ReLU eliminates the \"dead neuron\" danger\n",
    "    torch.nn.LeakyReLU(), \n",
    "    \n",
    "    # Might try another \"inference\" layer\n",
    "    torch.nn.Linear(hidden3, hidden4),\n",
    "    torch.nn.LeakyReLU(), \n",
    "\n",
    "    # A basic linear layer with one output for the \"continuous\" target \n",
    "    torch.nn.Linear(hidden4, out_dim),  \n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data, summarize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up the GPU\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Just the model itself:\")\n",
    "print(f\"{torch.cuda.memory_allocated():,} bytes allocated on GPU\")\n",
    "\n",
    "# Split the original data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Convert arrays to tensors\n",
    "X_train = torch.from_numpy(X_train.values).float().to(device)\n",
    "X_test  = torch.from_numpy(X_test.values).float().to(device)\n",
    "y_train = torch.from_numpy(y_train.values)[:, np.newaxis].float().to(device)\n",
    "y_test  = torch.from_numpy(y_test.values)[:, np.newaxis].float().to(device)\n",
    "\n",
    "print(\"Add the training and testing data to GPU:\")\n",
    "print(f\"{torch.cuda.memory_allocated():,} bytes allocated on GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, input_size=(1, X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "This will run for quite a few epochs.  On a good GPU, each epoch completes very quickly though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Now run model (start with high learning rate and decay)\n",
    "\n",
    "loss_fn = torch.nn.SmoothL1Loss()\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "do_training(model, X_train, y_train, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the predictions\n",
    "\n",
    "Our regression approach, we cannot determine any prediction probabilities.  In some ways, this is an advantage because we do not get discontinuities between a \"best guess\" and a \"second guess\" as we saw in the classification approach.  As a domain matter, the second guess should almost surely be \"a little bit smaller\" or a \"a little bit larger\" than the best guess.\n",
    "\n",
    "However, that simplification relies on our prior simplification of the target to ordinal values.  In the two dimensional sizing of the original garments, it is harder to say precisely what \"next largest\" or \"next smallest\" mean.  The visualization below simply makes a large number of predictions, and maps each point with an X-axis of the \"ground truth\" and a Y-axis of the prediction.  The predictions are continuous values, but those could easily be rounded to integers in the mapped range to make an actual garment size prediction.\n",
    "\n",
    "Notice that some predictions are of numeric values greater than 10 (the ordinal encoding of size `18`).  That would be fine with an rounding-to-ordinal rule though.  If the predictions were perfect, all the blue circles would lie on top of the red line.  The actual model is much worse than that.  It seems to do better than the classifier in some ways, but to a lesser degree also under-predicts the largest and smallest sizes, favoring middle sizes.  Moreover, the spread of predictions around the ground truth is not yet especially tight.\n",
    "\n",
    "Do you have ideas for approaches to improve these predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import randint\n",
    "labels = size_to_num.keys()\n",
    "\n",
    "ndxs = randint(0, len(X_test), 10_000)\n",
    "predictions = [p.item() for p in model(X_test[ndxs])]\n",
    "truths = [t.item() for t in y_test[ndxs]]\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(truths, predictions, marker='o', alpha=0.1)\n",
    "plt.xticks(range(len(labels)), labels, fontsize=8, rotation='vertical')\n",
    "plt.yticks(range(len(labels)), labels, fontsize=8)\n",
    "plt.xlabel(\"True size\")\n",
    "plt.ylabel(\"Predicted size\")\n",
    "ref = np.linspace(0, 10, 100);\n",
    "plt.plot(ref, ref, color=\"red\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Lesson\n",
    "\n",
    "**Tasks with Networks**: This lesson constructed a basic regression model, with moderately good success in its domain.  Next we will look at unsupervised learning and perform clustering with PyTorch neural networks.\n",
    "\n",
    "<a href=\"NetworkExamples_3.ipynb\"><img src=\"img/open-notebook.png\" align=\"left\"/></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
