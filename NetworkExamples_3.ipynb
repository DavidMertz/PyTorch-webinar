{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks with Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\">A simple feature classifier</font>\n",
    "<a href=\"NetworkExamples_0.ipynb\"><img src=\"img/open-notebook.png\" align=\"right\"/></a>\n",
    "\n",
    "<font size=\"+1\">An image classifier</font>\n",
    "<a href=\"NetworkExamples_1.ipynb\"><img src=\"img/open-notebook.png\" align=\"right\"/></a>\n",
    "\n",
    "<font size=\"+1\">A regression prediction</font>\n",
    "<a href=\"NetworkExamples_2.ipynb\"><img src=\"img/open-notebook.png\" align=\"right\"/></a>\n",
    "\n",
    "<font size=\"+1\"><u><b>Clustering with PyTorch</b></u></font>\n",
    "<a href=\"NetworkExamples_3.ipynb\"><img src=\"img/open-notebook.png\" align=\"right\"/></a>\n",
    "\n",
    "<font size=\"+1\">Generative Adversarial Networks (GAN)</font> \n",
    "<a href=\"NetworkExamples_4.ipynb\"><img src=\"img/open-notebook.png\" align=\"right\"/></a>\n",
    "\n",
    "<font size=\"+1\">Part of Speech Tagger</font>\n",
    "<a href=\"NetworkExamples_5.ipynb\"><img src=\"img/open-notebook.png\" align=\"right\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import math\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering with PyTorch\n",
    "\n",
    "We can implement the [K-means clustering algorithm](https://en.wikipedia.org/wiki/K-means_clustering) using PyTorch tensor arithmetic fairly easily.  Unlike other examples in this section, we do not utilize the `torch.nn` subpackage, and the calculation is not really a neural network.  Nonetheless, this kind of technique is common in machine learning, and is interesting to present here.\n",
    "\n",
    "Note that the technique given here **does not** guarantee convergence to a global optimum.  Choosing different initial points could quite possibly cause convergence to different centroids.  However, this approach is computationally tractable, while the global solution is NP-hard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise distance\n",
    "\n",
    "We will first write one support function to find the pairwise distances between all combinations of points in two tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distance(tensor1, tensor2=None, device=None):\n",
    "    \"\"\"Calculate pairwise Euclidian distance between tensors\n",
    "    \n",
    "    The input data are N*M tensors.  If the second tensor is omitted, we measure\n",
    "    self-distance of the first tensor. We calculate distance by:\n",
    "    \n",
    "    1. expand the N*M tensors into N*1*M and 1*N*M tensors\n",
    "    2. Perform a pairwise distance calculation\n",
    "    \"\"\"\n",
    "    # If second not given, calculate self-distance\n",
    "    tensor2 = tensor2 if tensor2 is not None else tensor1\n",
    "    A = tensor1.unsqueeze(dim=1)  # N x 1 x M\n",
    "    B = tensor2.unsqueeze(dim=0)  # 1 x N x M\n",
    "\n",
    "    # Our N*N tensor for pairwise distance\n",
    "    distances = (A-B)**2.0\n",
    "    distances = torch.sqrt(distances.sum(dim=-1).squeeze())\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to make sure it is clear what the PyTorch function is doing, let us break it down slightly to a scalar computation, and see what the pairwise version gives us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_3d(a, b):\n",
    "    \"For comparison, a single distance calculation\"\n",
    "    return math.sqrt((a.x - b.x)**2 + (a.y - b.y)**2 + (a.z - b.z)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Point3D = namedtuple('Point3D', 'x y z')\n",
    "a1, a2 = Point3D(1, 2, 3), Point3D(4, 5, 6)\n",
    "b1, b2 = Point3D(11, 12, 13), Point3D(14, 15, 16)\n",
    "\n",
    "print(\"A points:\", a1, a2)\n",
    "print(\"B points:\", b1, b2)\n",
    "print()\n",
    "print(\"From a1:\", distance_3d(a1, b1), distance_3d(a1, b2))\n",
    "print(\"From a2:\", distance_3d(a2, b1), distance_3d(a2, b2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.Tensor([a1, a2])\n",
    "B = torch.Tensor([b1, b2])\n",
    "\n",
    "pairwise_distance(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The K-means algorithm\n",
    "\n",
    "The general idea is that we select points closest to each candidate centroid, then repeatedly move the centroid to actually fall at the middle of the selected points.  Then on the next pass, we re-select those points that are closest to the adjusted centroid (which likely differs from those in the prior pass).  After a while this process converges, and we stop when one iteration of centroids differs very little (less than a tolerance) to the last iteration.\n",
    "\n",
    "A thing to notice about the K-means algorithm is that it works by finding [Voroni tesselations](https://en.wikipedia.org/wiki/Voronoi_diagram) of the parametric space.  Concretely, that means that the partitioning is into convex hulls, which limits those clusters that can be recognized.  However, in high dimensional parametric space, almost all distributions are well divided in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_points(X, n):\n",
    "    \"\"\"Given a tensor of observations select initial points for centroids\n",
    "    \n",
    "    This is the Forgy algorithm that simply chooses initial points from the data.\n",
    "    Other techniques include Random Partition, Maximin, and Bradley & Fayyad    \n",
    "    \"\"\"\n",
    "    indices = np.random.choice(len(X), n)\n",
    "    return X[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(X, n_clusters, tol=1e-5):\n",
    "    \"\"\"The K-means clustering algorithm, speficially Lloyd's algorithm\n",
    "    \n",
    "    Return both the clusters and the centroids\n",
    "    \"\"\"\n",
    "    centroids = initial_points(X, n_clusters)\n",
    "    \n",
    "    # Avoid repeatedly squaring the tensor elements\n",
    "    sqrt_tol = math.sqrt(tol)\n",
    "    \n",
    "    # Repeatedly adjust centroids until less than `tol` between each move\n",
    "    while True:\n",
    "        # Compute the distances from points to centroids\n",
    "        distances = pairwise_distance(X, centroids)\n",
    "        # Which centroid of this iteration is each point closest to?\n",
    "        clusters = torch.argmin(distances, dim=1)\n",
    "        # Ready for the next generation\n",
    "        centroids_pre = centroids.clone()\n",
    "\n",
    "        # Move each of the centrods\n",
    "        for ndx in range(n_clusters):\n",
    "            selected = torch.nonzero(clusters==ndx).squeeze()\n",
    "            selected = torch.index_select(X, 0, selected)\n",
    "            centroids[ndx] = selected.mean(dim=0)\n",
    "\n",
    "        # Measure the shift since last iteration, if small break\n",
    "        shift = torch.sum(torch.sqrt(torch.sum((centroids-centroids_pre)**2, dim=1)))\n",
    "        if shift < sqrt_tol:\n",
    "            break\n",
    "\n",
    "    # Return both the clusters and the centroids\n",
    "    return clusters, centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining clusters\n",
    "\n",
    "Some simple clusters are presented in the overview of machine learning.  Let us use those to look at this clustering.  First we look at all the raw points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from src.over_under_fit import cluster, blobs, show_clusters\n",
    "cluster(1, blobs=blobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let us see the \"ground truth\" of the generators of the clusters.  Obviously, in real observations the point is precisely that we do not have access to that ground truth and are trying to reconstruct it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster(1, known=True, blobs=blobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, let us see how the K-means implemented in scikit-learn divides the clusters. Notice that it is *mostly* but not *exactly* the same as the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster using scikit-learn k-means implemenation\n",
    "cluster(3, blobs=blobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally our PyTorch implementation.  For this we add the actual centroids selected as larger black circles.  Notice also that the selection of clusters is not quite identical to either the ground truth or the scikit-learn implementation.  Multiple runs of the next cell, in fact, may select slightly different clusters because the initial selection of centroids is randomized.  However, the behavior is very close in all cases, and the points that are ambiguous are also ambiguous to the human eye and judgement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clusters, centroids = kmeans(torch.from_numpy(blobs), 3)\n",
    "show_clusters(blobs, clusters, centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Lesson\n",
    "\n",
    "**Tasks with Networks**: This lesson constructed a clustering algorithm, KMeans, using the basic tensor math in PyTorch.  The next lesson will look at Generative Adversarial Networks.\n",
    "\n",
    "<a href=\"NetworkExamples_4.ipynb\"><img src=\"img/open-notebook.png\" align=\"left\"/></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
