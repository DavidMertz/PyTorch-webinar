{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks with Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\">A simple feature classifier</font>\n",
    "<a href=\"NetworkExamples_0.ipynb\"><img src=\"img/open-notebook.png\" align=\"right\"/></a>\n",
    "\n",
    "<font size=\"+1\">An image classifier</font>\n",
    "<a href=\"NetworkExamples_1.ipynb\"><img src=\"img/open-notebook.png\" align=\"right\"/></a>\n",
    "\n",
    "<font size=\"+1\">A regression prediction</font>\n",
    "<a href=\"NetworkExamples_2.ipynb\"><img src=\"img/open-notebook.png\" align=\"right\"/></a>\n",
    "\n",
    "<font size=\"+1\">Clustering with PyTorch</font>\n",
    "<a href=\"NetworkExamples_3.ipynb\"><img src=\"img/open-notebook.png\" align=\"right\"/></a>\n",
    "\n",
    "<font size=\"+1\"><u><b>Generative Adversarial Networks (GAN)</b></u></font> \n",
    "<a href=\"NetworkExamples_4.ipynb\"><img src=\"img/open-notebook.png\" align=\"right\"/></a>\n",
    "\n",
    "<font size=\"+1\">Part of Speech Tagger</font>\n",
    "<a href=\"NetworkExamples_5.ipynb\"><img src=\"img/open-notebook.png\" align=\"right\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Networks\n",
    "\n",
    "A GAN (Generative Adversarial Network) is a recent and powerful idea in design of neural networks.  While a GAN is technically a form of unsupervised learning, it cleverly captures much of the power of supervised learning models.  These models seem to have been used most widely in image generation contexts, but there is no reason they cannot be applied equally to other domains.  When applied to images, GAN's often produce \"surreal\" and sometimes disturbing resemblances to real images.\n",
    "\n",
    "For example, artist and A.I. enthusiast Robbie Barrat has produced these [images derived from painted nudes](https://twitter.com/DrBeef_/status/978732422085988352/photo/1?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E978732422085988352&ref_url=https%3A%2F%2Fwww.zmescience.com%2Fscience%2Fai-nudes-surreal-185131341%2F):\n",
    "\n",
    "![GAN Nudes](img/GAN-nudes.png)\n",
    "\n",
    "Or mentioned in this Martin Giles article in [MIT Technology Review](https://www.technologyreview.com/s/610253/the-ganfather-the-man-whos-given-machines-the-gift-of-imagination/) are these authentic seeming images of \"fake celebrities\" (computer generated images trained from many images of actual celebrities):\n",
    "\n",
    "![GAN celebs](img/GAN-celebs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea in a GAN is to run *two* neural networks in competition—hence the \"adversarial\" part of the name.  \n",
    "\n",
    "One neural network is a \"generator.\" Its goal is to generate new data that cannot be distinguished from genuine samples used to develop the GAN.  I.e. we **do** need to start with training datasets, but we do not have any known target feature that identifies correctness.  This is an unsupervised network, but correctness is defined by \"belonging to the training set\" as opposed to being any other (distribution of) possible values for the features.\n",
    "\n",
    "The other neural network is the \"discriminator.\" Its goal is to distinguish synthetic samples or observations from genuine ones.  The discriminator engages in a kind of supervised learning, since we the developers *do know* which image is which and can provide feedback to the discriminator.  While supervised models are very powerful, real world data is rarely trying actively to fool them about the class a datum belongs to.  In the GAN model, the adversary is specifically trying to outwit the classifier.\n",
    "\n",
    "Of course, there are some cases in the real world where fake data tries actively to pass itself off.  In forgery or fraud, a malicious actor is trying to create currency, or artwork, or some other item that can pass inspection by (human or machine) discriminators.  And many kinds of fraud involve trying to create transactions or messages that are difficult to distinguish from legitimate ones.  Unfortunately, GANs will probably be—in fact, probably already are—used to aid in some such fraud.\n",
    "\n",
    "This O'Reilly Press illustration is a good summary:\n",
    "\n",
    "![GAN schema](img/gan_schema.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A toy example\n",
    "\n",
    "For our sample code, we borrow and minimally change a GAN written by Dev Nag in his blog post [Generative Adversarial Networks (GANs) in 50 lines of code (PyTorch)](https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f).  Given that it is a toy, designed for simplicity of presentation, all this GAN is trying to learn is a Gaussian random distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import sigmoid, tanh, relu\n",
    "\n",
    "# For demonstration, we can use CPU target if CUDA not available\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Check the status of the GPU (if present)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.memory_allocated()\n",
    "    # *MUCH* faster to run on GPU\n",
    "    device = torch.device('cuda') \n",
    "    \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dataset\n",
    "\n",
    "First thing, initialize the dataset in our mentioned random distribution.  We have a number of choices about what \"features\" of the data we wish to model.  For this example, we use simply the first four moments of the data, but we could easily use the raw points, or other abstractions of the \"shape\" of the data, as we wished."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def decorate_with_diffs(data, exponent, remove_raw_data=False):\n",
    "    mean = torch.mean(data.data, 1, keepdim=True)\n",
    "    mean_broadcast = torch.mul(torch.ones(data.size()), mean.tolist()[0][0])\n",
    "    diffs = torch.pow(data - mean_broadcast, exponent)\n",
    "    if remove_raw_data:\n",
    "        return torch.cat([diffs], 1)\n",
    "    else:\n",
    "        return torch.cat([data, diffs], 1)\n",
    "    \n",
    "# Unused data features (experiment with these on your own).\n",
    "# Raw data\n",
    "preprocess, get_num_features = lambda data: data, lambda x: x\n",
    "# Data and variances\n",
    "preprocess, get_num_features = lambda data: decorate_with_diffs(data, 2.0), lambda x: x * 2\n",
    "# Data and diffs\n",
    "preprocess, get_num_features = lambda data: decorate_with_diffs(data, 1.0), lambda x: x * 2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moments(d):\n",
    "    # Return the first 4 moments of the data provided\n",
    "    mean = torch.mean(d)\n",
    "    diffs = d - mean\n",
    "    var = torch.mean(torch.pow(diffs, 2.0))\n",
    "    std = torch.pow(var, 0.5)\n",
    "    zscores = diffs / std\n",
    "    skews = torch.mean(torch.pow(zscores, 3.0))\n",
    "    kurtoses = torch.mean(torch.pow(zscores, 4.0)) - 3.0  # excess kurtosis, should be 0 for Gaussian\n",
    "    final = torch.cat((mean.reshape(1,), std.reshape(1,), skews.reshape(1,), kurtoses.reshape(1,)))\n",
    "    return final\n",
    "\n",
    "# Data points\n",
    "def d_sampler(n=500, mu=4, sigma=1.25):\n",
    "    \"Provide `n` random Gaussian distributed points with mean `mu` and std `sigma`\"\n",
    "    return torch.Tensor(np.random.normal(mu, sigma, n)).to(device)\n",
    "\n",
    "def gi_sampler(m=500, n=1):\n",
    "    \"Uniform-dist data into generator, NOT Gaussian\"\n",
    "    return torch.rand(m, n).to(device)\n",
    "\n",
    "preprocess = get_moments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(v):\n",
    "    return v.data.storage().tolist() \n",
    "\n",
    "def stats(v):\n",
    "    d = extract(v)\n",
    "    return [np.mean(d), np.std(d)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize a sample from the target distribution\n",
    "\n",
    "Let us quickly remind ourselves of what we are trying to imitate with the GAN.  This is a sample, and it will look slightly different each time we pull from distribution.  Notice in particular what the mean and spread are, which have to be learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(extract(d_sampler()), bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Models\n",
    "\n",
    "Define a generator and a discriminator in a standard fashion for PyTorch models.  Both have 3 linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, f):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "        self.f = f\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.map1(x)\n",
    "        x = self.dropout(x)  # Can we avoid a local trap?\n",
    "        x = self.f(x)\n",
    "        x = self.map2(x)\n",
    "        x = self.dropout(x)  # Can we avoid a local trap?\n",
    "        x = self.f(x)\n",
    "        x = self.map3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, f):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "        self.f = f\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.map1(x)\n",
    "        x = self.f(x)\n",
    "        x = self.map2(x)\n",
    "        x = self.f(x)\n",
    "        x = self.map3(x)\n",
    "        x = self.f(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate models, loss, and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "minibatch_size = 4\n",
    "num_epochs = 5000\n",
    "print_interval = 500\n",
    "d_steps = 20\n",
    "g_steps = 20\n",
    "\n",
    "G = Generator(input_size=1,   # Random noise dimension, per output vector\n",
    "              hidden_size=10, # Generator complexity\n",
    "              output_size=1,  # Size of generated output vector\n",
    "              f=relu          # Activation function\n",
    "             ).to(device)\n",
    "\n",
    "# Use input_size = get_num_features(...) if you try other examples\n",
    "D = Discriminator(input_size=4,   # 4 moments/features\n",
    "                  hidden_size=10, # Discriminator complexity\n",
    "                  output_size=1,  # Single dimension for 'real' vs. 'fake' classification\n",
    "                  f=sigmoid       # Activation function\n",
    "                 ).to(device)\n",
    "\n",
    "# Binary cross entropy: http://pytorch.org/docs/nn.html#bceloss\n",
    "criterion = nn.BCELoss()  \n",
    "\n",
    "# Stochastic Gradient Descent optimizers\n",
    "d_learning_rate = 1e-3\n",
    "g_learning_rate = 1e-3\n",
    "sgd_momentum = 0.9\n",
    "d_optimizer = optim.SGD(D.parameters(), lr=d_learning_rate, momentum=sgd_momentum)\n",
    "g_optimizer = optim.SGD(G.parameters(), lr=g_learning_rate, momentum=sgd_momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "During training we will show some information and visualization of the progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train(minibatch_size=500, g_input_size=1, d_input_size=500):\n",
    "    for epoch in range(num_epochs):\n",
    "        for d_index in range(d_steps):\n",
    "            # 1. Train D on real+fake\n",
    "            D.zero_grad()\n",
    "\n",
    "            #  1A: Train D on real\n",
    "            d_real_data = d_sampler(d_input_size)\n",
    "            d_real_decision = D(preprocess(d_real_data))\n",
    "            d_real_error = criterion(d_real_decision, torch.ones([1,1]).to(device))  # ones = true\n",
    "            d_real_error.backward() # compute/store gradients, but don't change params\n",
    "\n",
    "            #  1B: Train D on fake\n",
    "            d_gen_input = gi_sampler(minibatch_size, g_input_size)\n",
    "            d_fake_data = G(d_gen_input).detach()  # detach to avoid training G on these labels\n",
    "            d_fake_decision = D(preprocess(d_fake_data.t()))\n",
    "            d_fake_error = criterion(d_fake_decision, torch.zeros([1,1]).to(device))  # zeros = fake\n",
    "            d_fake_error.backward()\n",
    "            d_optimizer.step()     # Only optimizes D's parameters; changes based on stored gradients from backward()\n",
    "\n",
    "            dre, dfe = extract(d_real_error)[0], extract(d_fake_error)[0]\n",
    "\n",
    "        for g_index in range(g_steps):\n",
    "            # 2. Train G on D's response (but DO NOT train D on these labels)\n",
    "            G.zero_grad()\n",
    "\n",
    "            gen_input = gi_sampler(minibatch_size, g_input_size)\n",
    "            g_fake_data = G(gen_input)\n",
    "            dg_fake_decision = D(preprocess(g_fake_data.t()))\n",
    "            # Train G to pretend it's genuine\n",
    "            g_error = criterion(dg_fake_decision, torch.ones([1,1]).to(device))\n",
    "\n",
    "            g_error.backward()\n",
    "            g_optimizer.step()  # Only optimizes G's parameters\n",
    "            ge = extract(g_error)[0]\n",
    "\n",
    "        if epoch % print_interval == 0:\n",
    "            rstats, fstats = stats(d_real_data), stats(d_fake_data)\n",
    "            print(\"Epoch\", epoch, \"\\n\",\n",
    "                  \"  D: %.2f real_err, %.2f fake_err; G: %.2f err\\n\" % (dre, dfe, ge),\n",
    "                  \"  Real Dist: Mean: %.2f, Std: %.2f\\n\" % tuple(rstats),\n",
    "                  \"  Fake Dist: Mean: %.2f, Std: %.2f\" % tuple(fstats))\n",
    "\n",
    "            values = extract(g_fake_data)\n",
    "            plt.hist(values, bins=100)\n",
    "            plt.xlabel('Value')\n",
    "            plt.ylabel('Count')\n",
    "            plt.title('Histogram of Generated Distribution (epoch %d)' % epoch)\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pitfalls and guidelines\n",
    "\n",
    "When you train the discriminator, the generator will remain contant, and vice versa.  This gives each model a static adversary. If you have a roughly known domain, you might wish to pretrain the discriminator on similar data before starting your training of the generator.  This gives the generator a more difficult adversary to work against.\n",
    "\n",
    "Depending on the details of the network you configue, as well as other options in their training regimes, learning rates, optimizers, loss functions, and so on, one side of the GAN can overpower the other. If the discriminator is too good, it will return values  close to 0 or 1, and that the generator will be unable to find a meaningful gradient. If the generator is too good, it will exploit weaknesses in the discriminator that lead to false negatives. \n",
    "\n",
    "Dev Nag, in his blog post that I base this lesson on, present results from multiple runs of and identical GAN, mostly the same at the one in this notebook.  At times it does quite well, but at other times—just depending on randomized initial conditions—it does extremely poorly.  Sometimes additional training rounds may force them out of a poor local maximum, but often an unbalance is reached where progress is not possible.  I am *curious*, and explore it passingly above, whether addition of dropout layers or other layer engineering might mitigate this danger.\n",
    "\n",
    "![GAN generated distributions](img/GAN-generated-distributions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Lesson\n",
    "\n",
    "**Tasks with Networks**: This lesson examined Generative Adversarial Networks. The next lesson will create a part-of-speech tagger.\n",
    "\n",
    "<a href=\"NetworkExamples_5.ipynb\"><img src=\"img/open-notebook.png\" align=\"left\"/></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
